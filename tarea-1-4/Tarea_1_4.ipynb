{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYLGwJRK5GYe"
      },
      "source": [
        "# Análisis de sentimientos con NLP\n",
        "Vamos a utilizar Spacy y scikit-learn para clasificar con conjunto de tweets en español como positivos/negativos\n",
        "\n",
        "## Carga y preparación de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "PQbQKKtV5GYg",
        "outputId": "1b28c201-b95e-4cab-b913-3ccbd2ed1ee0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-Me caes muy bien \\r\\n-Tienes que jugar más partidas al lol con Russel y conmigo\\r\\n-Por qué tan Otako, deja de ser otako\\r\\n-Haber si me muero</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@myendlesshazza a. que puto mal escribo\\r\\n\\r\\nb. me sigo surrando help \\r\\n\\r\\n3. ha quedado raro el \"cómetelo\" ahí JAJAJAJA</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@estherct209 jajajaja la tuya y la d mucha gente seguro!! Pero yo no puedo sin mi melena me muero</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Quiero mogollón a @AlbaBenito99 pero sobretodo por lo rápido que contesta a los wasaps</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Vale he visto la tia bebiendose su regla y me hs dado muchs grima</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                           content  \\\n",
              "0  -Me caes muy bien \\r\\n-Tienes que jugar más partidas al lol con Russel y conmigo\\r\\n-Por qué tan Otako, deja de ser otako\\r\\n-Haber si me muero   \n",
              "1                    @myendlesshazza a. que puto mal escribo\\r\\n\\r\\nb. me sigo surrando help \\r\\n\\r\\n3. ha quedado raro el \"cómetelo\" ahí JAJAJAJA   \n",
              "2                                               @estherct209 jajajaja la tuya y la d mucha gente seguro!! Pero yo no puedo sin mi melena me muero    \n",
              "3                                                          Quiero mogollón a @AlbaBenito99 pero sobretodo por lo rápido que contesta a los wasaps    \n",
              "4                                                                               Vale he visto la tia bebiendose su regla y me hs dado muchs grima    \n",
              "\n",
              "  polarity  \n",
              "0     NONE  \n",
              "1        N  \n",
              "2        N  \n",
              "3        P  \n",
              "4        N  "
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_colwidth', None) # leer máximo ancho de columna\n",
        "\n",
        "# Leemos los datos\n",
        "data_path ='./tweets_all.csv'\n",
        "df = pd.read_csv(data_path, index_col=None)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThihT6_S5GYh",
        "outputId": "a9a7bdb6-d723-4766-c1c2-74c54a618700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1514 entries, 0 to 1513\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   content   1514 non-null   object\n",
            " 1   polarity  1514 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 23.8+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMWmyoh05GYh",
        "outputId": "76d4fa8c-1266-47b8-cd1c-c514be3997d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "N       637\n",
              "P       474\n",
              "NEU     202\n",
              "NONE    201\n",
              "Name: polarity, dtype: int64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.polarity.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrGqiLLO5GYh"
      },
      "source": [
        "Tenemos 1514 tweets, de los cuales hay 637 positivos y 474 negativos. El resto son neutros o no tienen polaridad clasificada.\n",
        "Vamos a entrenar sólo con los positivos y negativos para utilizar un clasificador binario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zUANnBFN5GYi"
      },
      "outputs": [],
      "source": [
        "df = df[(df['polarity']=='P') | (df['polarity']=='N')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWFmZqrY5GYi",
        "outputId": "c1d2a6fd-17c3-4621-a13a-83b5041cfddf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "N    637\n",
              "P    474\n",
              "Name: polarity, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.polarity.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (3.5.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: setuptools in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (61.2.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
            "Collecting es-core-news-md==3.3.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.3.0/es_core_news_md-3.3.0-py3-none-any.whl (42.3 MB)\n",
            "Collecting spacy<3.4.0,>=3.3.0.dev0\n",
            "  Using cached spacy-3.3.2-cp39-cp39-win_amd64.whl (11.7 MB)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (0.4.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (3.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.4.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (61.2.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (21.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.27.1)\n",
            "Collecting thinc<8.1.0,>=8.0.14\n",
            "  Using cached thinc-8.0.17-cp39-cp39-win_amd64.whl (1.0 MB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (6.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (1.0.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.0.8)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (0.7.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (1.21.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.0.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (8.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.0.1)\n",
            "Installing collected packages: thinc, spacy, es-core-news-md\n",
            "Successfully installed es-core-news-md-3.3.0 spacy-3.3.2 thinc-8.0.17\n",
            "✔ Download and installation successful\n",
            "You can now load the package via spacy.load('es_core_news_md')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script spacy.exe is installed in 'C:\\Users\\danie\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
          ]
        }
      ],
      "source": [
        "!pip install -U --user spacy\n",
        "!python -m spacy download es_core_news_md --user\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc0g2x_X5GYi"
      },
      "source": [
        "## Limpieza de texto\n",
        "Hacemos un pequeño pre-procesado del texto antes de extraer las características:  \n",
        "- Quitamos las menciones y las URL del texto porque no aportan valor para el análisis de sentimientos.\n",
        "- Los hashtag sí que pueden aportar valor así que simplemente quitamos el #.\n",
        "- Quitamos los signos de puntuación y palabras menores de 3 caracteres.\n",
        "- Por último quitamos todos los símbolos de puntuación del texto (que forman parte de un token).\n",
        "- Lematizamos el texto y lo guardamos en otra columna para comparar resultados del clasificador. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SCU2fEuR5GYi"
      },
      "outputs": [],
      "source": [
        "# Debes instalar las librerías necesarias\n",
        "import re, string, spacy\n",
        "nlp=spacy.load('es_core_news_md')  # carga el modelo en español es_core_news_md de la librería de spacy para hacer NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fCSK_orC5GYi"
      },
      "outputs": [],
      "source": [
        "# Lista de stop-words específicos de nuestro corpus (aproximación). Nota*: El corpus es el conjunto de textos que sirven como base para el análisis lingüístico\n",
        "stop_words = ['el', 'la', 'lo', 'los', 'las', 'un', 'una', 'unos', 'unas', 'me', 'a', 'de', 'se', 'te']\n",
        "\n",
        "pattern2 = re.compile('[{}]'.format(re.escape(string.punctuation))) #selecciona símbolos de puntuación\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Limpiamos las menciones y URL del texto. Luego convertimos todo en tokens,\n",
        "    eliminamos los tokens que son signos de puntuación y convertimos en\n",
        "    minúsculasn. Para terminar, volvemos a convertir en cadena de texto\"\"\"\n",
        "\n",
        "    text = re.sub(r'@[\\w_]+|https?://[\\w_./]+', '', text) #elimina menciones y URL\n",
        "    tokens = nlp(text)\n",
        "    tokens = [tok.lower_ for tok in tokens if not tok.is_punct and not tok.is_space]\n",
        "    filtered_tokens = [pattern2.sub('', token) for token in tokens if not (token in stop_words)] #obvia stop_words y después quita signos de puntuación\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    \n",
        "    return filtered_text\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    \"\"\"Convertimos el texto a tokens, extraemos el lexema de cada token\n",
        "    y volvemos a convertir en cadena de texto\"\"\"\n",
        "    tokens = nlp(text)\n",
        "    lemmatized_tokens = [tok.lemma_ for tok in tokens]\n",
        "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "    \n",
        "    return lemmatized_text\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B_aAaRz5GYj"
      },
      "source": [
        "Probamos el funcionamiento de estas funciones sobre un tweet de ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4Qj8hOs5GYj",
        "outputId": "ecd4eb40-250a-43d8-ad8d-0ad91e0be212"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:\n",
            " Mg y pongo un adjetivo super repelente a vuestro nombre \n",
            "\n",
            "Limpiado:\n",
            " mg y pongo adjetivo super repelente vuestro nombre\n",
            "\n",
            "Lematizado:\n",
            " mg y poner adjetivo super repelente vuestro nombre\n"
          ]
        }
      ],
      "source": [
        "print('Original:\\n',df.content[10])\n",
        "print('\\nLimpiado:\\n',clean_text(df.content[10]))\n",
        "print('\\nLematizado:\\n',lemmatize_text(clean_text(df.content[10])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSNgd86O5GYj"
      },
      "source": [
        "Aplicamos limpieza a todos los tweets del DataFrame y creamos columna nueva con los lemas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "syK0P5hZ5GYj"
      },
      "outputs": [],
      "source": [
        "df.content=df.content.apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oFrFlRJ15GYk"
      },
      "outputs": [],
      "source": [
        "#Quitamos tweets vacíos después de la limpieza\n",
        "df=df[df.content!='']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZW4fvE3l5GYk"
      },
      "outputs": [],
      "source": [
        "df[\"lemas\"]=df.content.apply(lemmatize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MCzHyWyu5GYk"
      },
      "outputs": [],
      "source": [
        "#Contamos el nº de palabras por tweet\n",
        "df['content_words'] = [len(t.split(' ')) for t in df.content]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNEvewfV5GYk",
        "outputId": "c41d9502-d7f6-4617-b322-1afdd16b954d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       content_words\n",
            "count    1111.000000\n",
            "mean       12.166517\n",
            "std         4.764943\n",
            "min         3.000000\n",
            "25%         8.000000\n",
            "50%        12.000000\n",
            "75%        16.000000\n",
            "max        26.000000\n",
            "                                                                               content  \\\n",
            "1  a que puto mal escribo b sigo surrando help 3 ha quedado raro cómetelo ahí jajajaja   \n",
            "2            jajajaja tuya y d mucha gente seguro pero yo no puedo sin mi melena muero   \n",
            "3                        quiero mogollón pero sobretodo por rápido que contesta wasaps   \n",
            "4                          vale he visto tia bebiendose su regla y hs dado muchs grima   \n",
            "5                      ah mucho más por supuesto solo que incluyo habías entendido mal   \n",
            "\n",
            "  polarity  \\\n",
            "1        N   \n",
            "2        N   \n",
            "3        P   \n",
            "4        N   \n",
            "5        P   \n",
            "\n",
            "                                                                                     lemas  \\\n",
            "1  a que puto mal escribir b seguir surrar help 3 haber quedar raro cómetelo ahí jajajajar   \n",
            "2               jajajajar tuya y d mucho gente seguro pero yo no poder sin mi melena muero   \n",
            "3                          querer mogollón pero sobretodir por rápido que contestar wasaps   \n",
            "4                              valer haber ver tia beber él su regla y hs dado muchs grima   \n",
            "5                            ah mucho más por supuesto solo que incluyo haber entender mal   \n",
            "\n",
            "   content_words  \n",
            "1             16  \n",
            "2             15  \n",
            "3              9  \n",
            "4             12  \n",
            "5             11  \n"
          ]
        }
      ],
      "source": [
        "print(df.describe())\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGtOtXcV5GYk"
      },
      "source": [
        "Vemos que de media cada tweet tiene unas 12 palabras, con un máximo de 26 palabras.  \n",
        "### Clasificador\n",
        "Vamos a usar la librería scikit-learn para aplicar un clasificador binario sobre la polaridad. Aplicamos dos modelos distintos para extraer las características del texto, Bag-of-Words (BoW) y Term Frequency times Inverse Document Frequency (TF-IDF).  \n",
        "\n",
        "Primero dividimos en conjunto de entrenamiento y test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "b9VOZOkB5GYk"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and test sets\n",
        "# Asignamos un 70% a training y un 30% a test\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['content'], \n",
        "                                                    df['polarity'],\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F7IrfTZ5GYl",
        "outputId": "6f43bc62-ab71-41d8-a3c9-9f351f1fa998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Primera entrada de train:\n",
            " estoy preparando muchas cosas para que especial sea muy especial\n",
            "Polaridad: P\n",
            "\n",
            "X_train shape: (777,)\n",
            "\n",
            "X_test shape: (334,)\n"
          ]
        }
      ],
      "source": [
        "print('Primera entrada de train:\\n', X_train.iloc[0])\n",
        "print('Polaridad:', y_train.iloc[0])\n",
        "print('\\nX_train shape:', X_train.shape)\n",
        "print('\\nX_test shape:', X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swqSr2q-5GYl"
      },
      "source": [
        "## Modelo Bag of Words\n",
        "El BoW se implementa con la función `CountVectorizer` de `scikit-learn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGOpGiG25GYl"
      },
      "source": [
        "El objetivo de la bolsa de palabras es transformar el texto en una representación numérica que se pueda utilizar en algoritmos de aprendizaje automático. El proceso implica los siguientes pasos:\n",
        "\n",
        "Tokenización: El texto se divide en tokens o palabras individuales.\n",
        "\n",
        "Limpieza: Se eliminan las palabras vacías (stopwords) y se realizan otras operaciones de limpieza del texto como la lematización.\n",
        "\n",
        "Creación del vocabulario: Se crea un vocabulario de todas las palabras únicas que aparecen en el conjunto de documentos.\n",
        "\n",
        "Conteo de frecuencia: Se cuenta el número de veces que cada palabra aparece en cada documento.\n",
        "\n",
        "Creación de la matriz de características: Se crea una matriz que representa cada documento como un vector de características, donde cada característica corresponde a una palabra del vocabulario y su valor es el número de veces que esa palabra aparece en el documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "hUyqgAjb5GYl",
        "outputId": "2a72fed1-e5e6-4e72-e7fe-3e67458bdbee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CountVectorizer()"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# aprendemos el modelo CountVectorizer sobre el conjunto de train\n",
        "vect = CountVectorizer()\n",
        "vect.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPKbK4Tn5GYl"
      },
      "source": [
        "Vemos el número de términos distintos que tiene el diccionario:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTHZoHy55GYl",
        "outputId": "9e4b185d-b465-4861-bb2b-e0c110fcc636"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3277"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vect.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnaWdsEO5GYl"
      },
      "source": [
        "Creamos la matriz BoW del conjunto de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO6BzoPd5GYl",
        "outputId": "381cec46-0937-4728-b236-01478e2bacd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(777, 3277)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# transformamos documentos de train en matriz de características\n",
        "X_train_vectorized = vect.transform(X_train)\n",
        "\n",
        "train=X_train_vectorized.toarray()\n",
        "print(train.shape)\n",
        "sum(train[:][0])  # hay diez elementos encontrados para la característica (feature) primera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOZGOAn25GYl"
      },
      "source": [
        "### Entrenamiento del modelo\n",
        "Vamos a probar un clasificador Logistic Regression de scikit-learn para entrenar nuestro modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "4CqGYdhc5GYm",
        "outputId": "e7780f02-cd65-49df-8628-ffc11eeb1699"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LogisticRegression(solver='liblinear')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "modelLR = LogisticRegression(solver='liblinear')\n",
        "#Entrenamos el modelo con el conjunto de train\n",
        "modelLR.fit(X_train_vectorized, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ2TPgCl5GYm"
      },
      "source": [
        "### Verificación del modelo\n",
        "Para ver el rendimiento del modelo usamos el conjunto de test. Primero transformamos el conjunto de test a su matriz BoW mediante el vectorizador aprendido en TRAIN y aplicamos el modelo entrenado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et7t3fbi5GYm",
        "outputId": "6d21ad38-09aa-48c9-ac62-790f3a63754b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(334, 3277)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Predecimos sobre el conjunto de test\n",
        "X_test_vectorized = vect.transform(X_test)\n",
        "X_test_vectorized.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py659bRR5GYm",
        "outputId": "882f8cfc-8be3-424d-80fa-9a80abebff1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['N' 'N' 'N' 'P' 'N' 'N' 'P' 'N' 'P' 'N' 'N' 'P' 'P' 'P' 'N' 'N' 'N' 'P'\n",
            " 'P' 'P' 'N' 'P' 'P' 'P' 'N' 'N' 'N' 'P' 'P' 'N' 'P' 'P' 'P' 'P' 'N' 'N'\n",
            " 'N' 'N' 'N' 'P' 'P' 'N' 'N' 'N' 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'P' 'P' 'P'\n",
            " 'P' 'P' 'N' 'P' 'N' 'N' 'P' 'N' 'P' 'P' 'N' 'N' 'P' 'N' 'P' 'N' 'P' 'P'\n",
            " 'P' 'N' 'N' 'N' 'P' 'P' 'N' 'N' 'N' 'P' 'P' 'P' 'N' 'N' 'N' 'P' 'N' 'N'\n",
            " 'N' 'N' 'N' 'N' 'N' 'P' 'N' 'P' 'N' 'P' 'N' 'N' 'P' 'N' 'N' 'N' 'P' 'N'\n",
            " 'P' 'N' 'P' 'P' 'N' 'N' 'N' 'P' 'P' 'N' 'N' 'N' 'N' 'N' 'P' 'N' 'P' 'N'\n",
            " 'N' 'N' 'P' 'N' 'P' 'N' 'P' 'N' 'N' 'P' 'N' 'P' 'N' 'P' 'N' 'N' 'N' 'N'\n",
            " 'P' 'N' 'P' 'P' 'P' 'P' 'P' 'N' 'P' 'N' 'N' 'P' 'P' 'P' 'P' 'N' 'N' 'N'\n",
            " 'P' 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'P' 'P' 'N' 'P' 'N' 'P' 'N' 'N' 'P' 'P'\n",
            " 'P' 'P' 'P' 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'P'\n",
            " 'P' 'P' 'N' 'N' 'N' 'P' 'N' 'N' 'N' 'P' 'P' 'P' 'N' 'N' 'N' 'N' 'N' 'N'\n",
            " 'N' 'P' 'P' 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'N' 'P' 'N' 'P' 'N' 'N' 'P' 'N'\n",
            " 'N' 'P' 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'P' 'P' 'P' 'N' 'P' 'N' 'N' 'N' 'P'\n",
            " 'N' 'N' 'N' 'P' 'P' 'P' 'P' 'N' 'N' 'P' 'N' 'N' 'N' 'P' 'N' 'P' 'N' 'N'\n",
            " 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'P' 'N' 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'P' 'N'\n",
            " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'P' 'P' 'N' 'N' 'P' 'N' 'N' 'P' 'P' 'N'\n",
            " 'P' 'N' 'P' 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
            " 'N' 'N' 'N' 'P' 'P' 'P' 'N' 'N' 'N' 'P']\n"
          ]
        }
      ],
      "source": [
        "prediccion = modelLR.predict(X_test_vectorized)\n",
        "print(prediccion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE409oAC5GYm"
      },
      "source": [
        "Vemos el resultado de la predicción y calculamos su precisión con distintas métricas.  \n",
        "Ejemplo de predicción de algunas muestras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "53EwpQMK5GYm",
        "outputId": "1b76c675-2a02-4eed-b21b-18d2bdda13ce"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texto</th>\n",
              "      <th>polaridad</th>\n",
              "      <th>predicción</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>totalmente pobre gappy que inocente es no sabe con quien ha encontrado</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1246</th>\n",
              "      <td>ha explotado vaso en mano que forma tan bonita empezar día</td>\n",
              "      <td>N</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1033</th>\n",
              "      <td>que kinox no quiere zi yo zoy buena perzona</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1191</th>\n",
              "      <td>isco con juande no seria titular y sabeis al mister le van mas tissones</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>he capturado mi primer gimnasio en pokemon go cierto es que es probable que no tarden ni 10 minutos en quitarmelo pero mola</td>\n",
              "      <td>P</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1373</th>\n",
              "      <td>mi padre le ha dado manotazo mi móvil y gracias dios que que ha roto ha sido cristal templado</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>ojalá justin subiendo foto comiéndose boca con sofía para que os den por culo todas</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>645</th>\n",
              "      <td>tiene que ser entretenido tu seras primera en verlo amore mio 😘</td>\n",
              "      <td>P</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1261</th>\n",
              "      <td>yo puedo cambiar opinión cara es más difícil porque vale pasta</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>941</th>\n",
              "      <td>y que sea yo es triste</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                            texto  \\\n",
              "134                                                        totalmente pobre gappy que inocente es no sabe con quien ha encontrado   \n",
              "1246                                                                   ha explotado vaso en mano que forma tan bonita empezar día   \n",
              "1033                                                                                  que kinox no quiere zi yo zoy buena perzona   \n",
              "1191                                                      isco con juande no seria titular y sabeis al mister le van mas tissones   \n",
              "799   he capturado mi primer gimnasio en pokemon go cierto es que es probable que no tarden ni 10 minutos en quitarmelo pero mola   \n",
              "1373                                mi padre le ha dado manotazo mi móvil y gracias dios que que ha roto ha sido cristal templado   \n",
              "481                                           ojalá justin subiendo foto comiéndose boca con sofía para que os den por culo todas   \n",
              "645                                                               tiene que ser entretenido tu seras primera en verlo amore mio 😘   \n",
              "1261                                                               yo puedo cambiar opinión cara es más difícil porque vale pasta   \n",
              "941                                                                                                        y que sea yo es triste   \n",
              "\n",
              "     polaridad predicción  \n",
              "134          N          N  \n",
              "1246         N          P  \n",
              "1033         N          N  \n",
              "1191         N          N  \n",
              "799          P          N  \n",
              "1373         N          N  \n",
              "481          N          N  \n",
              "645          P          P  \n",
              "1261         N          N  \n",
              "941          N          N  "
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame({'texto':X_test, 'polaridad':y_test, 'predicción':prediccion}).sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otE1kSco5GYm"
      },
      "source": [
        "Precisión del modelo (# predicciones correctas / Total de muestras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJgOTzFl5GYm",
        "outputId": "b805b5e3-e6c0-4055-f888-f8c13fd5ea78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy (exactitud):  0.7215568862275449\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print('Accuracy (exactitud): ', accuracy_score(y_test, prediccion))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E71O0a0b5GYm"
      },
      "source": [
        "Matriz de confusión (predicción -columnas- frente a etiquetas reales -filas-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "Yn0ij_A15GYn",
        "outputId": "472e7879-bebb-4ab5-bedb-c5f57e58fde8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>N_pred</th>\n",
              "      <th>P_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>N_true</th>\n",
              "      <td>154</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>P_true</th>\n",
              "      <td>55</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        N_pred  P_pred\n",
              "N_true     154      38\n",
              "P_true      55      87"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test, prediccion)\n",
        "pd.DataFrame(cm, index=('N_true','P_true'), columns=('N_pred','P_pred'))\n",
        "#filas: True Label, columnas: Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtAWaT0u5GYn"
      },
      "source": [
        "### Veamos qué palabras son las más relevantes en el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmEcrYaa5GYn",
        "outputId": "c45a2ee3-d374-42ca-93a6-8cb9cf4bd809"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Menores Coefs:\n",
            "['ni' 'no' 'puto' 'triste' 'alguien' 'puta' 'sad' 'porque' 'algún' 'eso']\n",
            "\n",
            "Mayores Coefs: \n",
            "['gran' 'gracias' 'genial' 'buena' 'bien' 'hacerlo' 'puedes' 'guapa'\n",
            " 'muchas' 'tranquila']\n"
          ]
        }
      ],
      "source": [
        "# obtenemos los nombres de las características numpy array\n",
        "feature_names = np.array(vect.get_feature_names_out())\n",
        "\n",
        "# Ordenamos los coeficientes del modelo\n",
        "sorted_coef_index = modelLR.coef_[0].argsort()\n",
        "\n",
        "# Listamos los 10 coeficientes menores y mayores\n",
        "print('Menores Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
        "print('Mayores Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdrU8yM35GYn"
      },
      "source": [
        "## Otros modelos\n",
        "Probamos con los modelos Naïve Bayes y un SGD lineal para ver si mejora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIZNi_-y5GYn"
      },
      "source": [
        "Los modelos MultinomialNB (Multinomial Naive Bayes) son una técnica de aprendizaje automático que se utiliza principalmente en problemas de clasificación de texto.\n",
        "El modelo SGDClassifier es un modelo de clasificación lineal que utiliza el método de descenso de gradiente estocástico como método de optimización para ajustar los pesos del modelo en el entrenamiento, por defecto aplica SVM (support vector machines)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "N2U2QUSM5GYn"
      },
      "outputs": [],
      "source": [
        "def train_predict_evaluate_model(classifier, \n",
        "                                 train_features, train_labels, \n",
        "                                 test_features, test_labels):\n",
        "    '''Función que entrena y valida un clasificador sobre\n",
        "    un conjunto especificado de entrenamiento y test.\n",
        "    Devuelve la predicción sobre el conjunto de test'''\n",
        "    # entrena modelo    \n",
        "    classifier.fit(train_features, train_labels)\n",
        "    # predice en test usando el modelo\n",
        "    predictions = classifier.predict(test_features) \n",
        "    # evalúa el rendimiento del modelo   \n",
        "    print('Accuracy (exactitud): ', accuracy_score(test_labels, predictions))\n",
        "    return predictions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXgcSa0t5GYn",
        "outputId": "04e3cc6b-3947-4167-fc5b-26328fab8d54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo Multinomial Naïve Bayes:\n",
            "Accuracy (exactitud):  0.7514970059880239\n",
            "Modelo SVM lineal:\n",
            "Accuracy (exactitud):  0.6706586826347305\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.naive_bayes import MultinomialNB  # Multinomial naive bayes\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# creamos los modelos\n",
        "modelNB = MultinomialNB()\n",
        "modelSVM = SGDClassifier(loss='hinge', max_iter=10000, tol=1e-5) # loss/cost function, tol es el error a partir del cual deja de entrenar\n",
        "\n",
        "# entrenamos y evaluamos\n",
        "X_test_vectorized=vect.transform(X_test)\n",
        "print(\"Modelo Multinomial Naïve Bayes:\")\n",
        "prediccionNB = train_predict_evaluate_model(modelNB, \n",
        "                                 X_train_vectorized, y_train, \n",
        "                                 X_test_vectorized, y_test)\n",
        "print(\"Modelo SVM lineal:\")\n",
        "prediccionSVM = train_predict_evaluate_model(modelSVM, \n",
        "                                 X_train_vectorized, y_train, \n",
        "                                 X_test_vectorized, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCjrF2hg5GYo"
      },
      "source": [
        "### Ejercicio 1:\n",
        "Vuelve a entrenar el modelo Logistic Regression usando la función `train_predict_evaluate_model` para comparar fácilmente con los resultados anteriores. ¿cuál funciona mejor de los tres?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94MSti4R5GYo",
        "outputId": "c948675d-8074-46ee-c989-6ef944bc6961"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy (exactitud):  0.7215568862275449\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array(['N', 'N', 'N', 'P', 'N', 'N', 'P', 'N', 'P', 'N', 'N', 'P', 'P',\n",
              "       'P', 'N', 'N', 'N', 'P', 'P', 'P', 'N', 'P', 'P', 'P', 'N', 'N',\n",
              "       'N', 'P', 'P', 'N', 'P', 'P', 'P', 'P', 'N', 'N', 'N', 'N', 'N',\n",
              "       'P', 'P', 'N', 'N', 'N', 'N', 'N', 'P', 'N', 'N', 'N', 'N', 'P',\n",
              "       'P', 'P', 'P', 'P', 'N', 'P', 'N', 'N', 'P', 'N', 'P', 'P', 'N',\n",
              "       'N', 'P', 'N', 'P', 'N', 'P', 'P', 'P', 'N', 'N', 'N', 'P', 'P',\n",
              "       'N', 'N', 'N', 'P', 'P', 'P', 'N', 'N', 'N', 'P', 'N', 'N', 'N',\n",
              "       'N', 'N', 'N', 'N', 'P', 'N', 'P', 'N', 'P', 'N', 'N', 'P', 'N',\n",
              "       'N', 'N', 'P', 'N', 'P', 'N', 'P', 'P', 'N', 'N', 'N', 'P', 'P',\n",
              "       'N', 'N', 'N', 'N', 'N', 'P', 'N', 'P', 'N', 'N', 'N', 'P', 'N',\n",
              "       'P', 'N', 'P', 'N', 'N', 'P', 'N', 'P', 'N', 'P', 'N', 'N', 'N',\n",
              "       'N', 'P', 'N', 'P', 'P', 'P', 'P', 'P', 'N', 'P', 'N', 'N', 'P',\n",
              "       'P', 'P', 'P', 'N', 'N', 'N', 'P', 'N', 'N', 'P', 'N', 'N', 'N',\n",
              "       'N', 'P', 'P', 'N', 'P', 'N', 'P', 'N', 'N', 'P', 'P', 'P', 'P',\n",
              "       'P', 'N', 'N', 'P', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N',\n",
              "       'N', 'N', 'P', 'P', 'P', 'N', 'N', 'N', 'P', 'N', 'N', 'N', 'P',\n",
              "       'P', 'P', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'P', 'P', 'N', 'N',\n",
              "       'P', 'N', 'N', 'N', 'N', 'N', 'P', 'N', 'P', 'N', 'N', 'P', 'N',\n",
              "       'N', 'P', 'N', 'N', 'P', 'N', 'N', 'N', 'N', 'P', 'P', 'P', 'N',\n",
              "       'P', 'N', 'N', 'N', 'P', 'N', 'N', 'N', 'P', 'P', 'P', 'P', 'N',\n",
              "       'N', 'P', 'N', 'N', 'N', 'P', 'N', 'P', 'N', 'N', 'N', 'N', 'P',\n",
              "       'N', 'N', 'N', 'N', 'P', 'N', 'N', 'N', 'P', 'N', 'N', 'N', 'N',\n",
              "       'P', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'P', 'P', 'N',\n",
              "       'N', 'P', 'N', 'N', 'P', 'P', 'N', 'P', 'N', 'P', 'N', 'N', 'P',\n",
              "       'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N',\n",
              "       'N', 'N', 'P', 'P', 'P', 'N', 'N', 'N', 'P'], dtype=object)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### SOLUCIÓN\n",
        "# Crea el modelo de regresión logística\n",
        "modelLR = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# Entrena y evalúa el modelo utilizando la función train_predict_evaluate_model\n",
        "train_predict_evaluate_model(modelLR, X_train_vectorized, y_train, X_test_vectorized, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVhWJYjEF7m4"
      },
      "source": [
        "El Multinomial Naïve Bayes es el que ha dado mejores resultados de los tres, pero aún así son todos muy parecidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIGUl2q15GYo"
      },
      "source": [
        "## Modelo TF-IDF\n",
        "\n",
        "El modelo TF-IDF asigna un peso a cada palabra en un documento en función de su frecuencia en el documento y en todo el corpus. La idea detrás del modelo es que las palabras que aparecen con frecuencia en un documento pero raramente en el corpus en general tienen un mayor peso y, por lo tanto, son más importantes para ese documento en particular. Las palabras que aparecen con frecuencia en el corpus en general pero raramente en el documento, tienen un peso menor y, por lo tanto, se consideran menos importantes para ese documento en particular.\n",
        "\n",
        "EL modelo TF-IDF se implementa con la función `TfidfVectorizer` de `scikit-learn`. Definimos una función para calcular el modelo TF-IDF y extraer las características del conjunto de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "vQD8QpLF5GYo"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#definimos una función para ajustar el modelo y extraer las características (palabras de la matriz)\n",
        "def tfidf_extractor(corpus):\n",
        "    '''Función que genera un modelo TF-IDF sobre un corpus de texto\n",
        "    El corpus debe ser una lista de textos (palabras separadas por espacios)\n",
        "    Devuelve el modelo TF-IDF generado y el vector TF-IDF del corpus'''\n",
        "    \n",
        "    vectorizer = TfidfVectorizer()\n",
        "    features = vectorizer.fit_transform(corpus)\n",
        "    return vectorizer, features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6E0X2fi5GYo"
      },
      "source": [
        "Calculamos el modelo TF-IDF sobre el corpus de entrenamiento y con este modelo extraemos las matrices del conjunto de entrenamiento y de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "aR4HIM0D5GYo"
      },
      "outputs": [],
      "source": [
        "#Creamos los vectores de características TF-IDF\n",
        "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(X_train)  \n",
        "tfidf_test_features = tfidf_vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-amDQ7J5GYo",
        "outputId": "fce4745e-1f66-4a68-fc29-6d80fcae0aa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo Logistic Regression con características TF-IDF\n",
            "Accuracy (exactitud):  0.6946107784431138\n",
            "Modelo Naive Bayes con características TF-IDF\n",
            "Accuracy (exactitud):  0.6916167664670658\n",
            "Modelo Linear SVM con características TF-IDF\n",
            "Accuracy (exactitud):  0.718562874251497\n"
          ]
        }
      ],
      "source": [
        "#Entrenamos los 3 clasificadores con las características TF-IDF\n",
        "modelos = [('Logistic Regression', modelLR),\n",
        "           ('Naive Bayes', modelNB),\n",
        "           ('Linear SVM', modelSVM)]\n",
        "for m, clf in modelos:\n",
        "    print('Modelo {} con características TF-IDF'.format(m))\n",
        "    tfidf_predictions = train_predict_evaluate_model(classifier=clf,\n",
        "                                           train_features=tfidf_train_features,\n",
        "                                           train_labels=y_train,\n",
        "                                           test_features=tfidf_test_features,\n",
        "                                           test_labels=y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0T8XB8J5GYo"
      },
      "source": [
        "Obtenemos unos resultados algo peores a los que obtenemos con los modelos BoW.  \n",
        "Los mayores coeficientes para cada clase son:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp1dM7K_5GYo",
        "outputId": "3943e1ac-85bf-48c4-8378-7be64cc12c29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Menores Coefs:\n",
            "['no' 'porque' 'ni' 'estoy' 'eso' 'triste' 'he' 'sad' 'puta' 'puto']\n",
            "\n",
            "Mayores Coefs: \n",
            "['gracias' 'gran' 'feliz' 'día' 'buena' 'genial' 'bien' 'buen' 'mejor'\n",
            " 'guapa']\n"
          ]
        }
      ],
      "source": [
        "# obtenemos los nombres de las características numpy array\n",
        "feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Ordenamos los coeficientes del modelo\n",
        "sorted_coef_index = modelLR.coef_[0].argsort()  \n",
        "\n",
        "# Listamos los 10 coeficientes menores y mayores\n",
        "print('Menores Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
        "\n",
        "print('Mayores Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]])) # 10 últimos elementos, -1 significa en orden inverso\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlh5rean5GYp",
        "outputId": "f37c81dc-062b-4fa4-90d7-2ab16a660d32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(777, 3277)\n",
            "(334, 3277)\n"
          ]
        }
      ],
      "source": [
        "# Convierto tfidf_train_features a array para visualizar su contenido y Comprueba las dimensiones de las los conjuntos de características de entrenamiento y test\n",
        "train= tfidf_train_features.toarray()\n",
        "print(tfidf_train_features.shape)\n",
        "print(tfidf_test_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuManKVL5GYp"
      },
      "source": [
        "### Ejercicio 2:\n",
        "Crea una función `bow_extractor` análoga a la función `tfidf_extractor` para generar un modelo BoW a partir de un corpus de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "If2Uuzy_5GYp"
      },
      "outputs": [],
      "source": [
        "#definimos una función para ajustar el modelo y extraer las características (palabras de la matriz)\n",
        "def bow_extractor(corpus):\n",
        "    '''Función que genera un modelo BoW sobre un corpus de texto\n",
        "    El corpus debe ser una lista de textos (palabras separadas por espacios)\n",
        "    Devuelve el modelo BoW generado y el vector BoW del corpus'''\n",
        "    \n",
        "    vectorizer = CountVectorizer()\n",
        "    features = vectorizer.fit_transform(corpus)\n",
        "    return vectorizer, features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3D0oytw5GYp"
      },
      "source": [
        "## Modelos sobre texto lematizado\n",
        "Probamos a entrenar los clasificadores con el texto lematizado para ver si mejoramos los resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "sd_amIe65GYp"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['lemas'], \n",
        "                                                    df['polarity'],\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtOXGtVQ5GYp"
      },
      "source": [
        "### Ejercicio 3:\n",
        "Aplica los modelos de BoW y TF-IDF con las funciones definidas anteriormente para obtener las matrices de características del conjunto de entrenamiento y de test (texto lematizado). Tienes que calcular las matrices `bow_train_features` y `bow_test_features` con el modelo BoW y las matrices `tfidf_train_features` y `tfidf_test_features` sobre el modelo TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "lxz9ulXn5GYp"
      },
      "outputs": [],
      "source": [
        "## SOLUCIÓN\n",
        "#Modelo BoW\n",
        "bow_vectorizer, bow_train_features = tfidf_extractor(X_train)  \n",
        "bow_test_features = bow_vectorizer.transform(X_test)\n",
        "\n",
        "#Modelo TF-IDF\n",
        "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(X_train)  \n",
        "tfidf_test_features = tfidf_vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED6pOFcb5GYp",
        "outputId": "758b504d-7194-4125-91cc-a8e8dcd764c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2618\n",
            "2618\n"
          ]
        }
      ],
      "source": [
        "print(len(bow_vectorizer.get_feature_names_out()))\n",
        "print(len(tfidf_vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rUVtKnP5GYp"
      },
      "source": [
        "Observa que el número de términos en el vocabulario se ha reducido notablemente al coger el lema de las palabras (muchos términos compartían el mismo lema).  \n",
        "Probamos si se mejora con los 3 clasificadores que hemos usado anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97UBxeNg5GYp",
        "outputId": "fda8b995-0834-43d4-ce17-2d6f6d190748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo Logistic Regression con características BoW\n",
            "Accuracy (exactitud):  0.7604790419161677\n",
            "Modelo Naive Bayes con características BoW\n",
            "Accuracy (exactitud):  0.7365269461077845\n",
            "Modelo Linear SVM con características BoW\n",
            "Accuracy (exactitud):  0.7125748502994012\n"
          ]
        }
      ],
      "source": [
        "#entrenamos clasificadores con modelos BoW\n",
        "for m, clf in modelos:\n",
        "    print('Modelo {} con características BoW'.format(m))\n",
        "    bow_predictions = train_predict_evaluate_model(classifier=clf,\n",
        "                                           train_features=bow_train_features,\n",
        "                                           train_labels=y_train,\n",
        "                                           test_features=bow_test_features,\n",
        "                                           test_labels=y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-bM9e7S5GYq"
      },
      "source": [
        "Haz lo mismo para TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Esk9Wppi5GYq",
        "outputId": "1457c465-1750-4f17-9bbb-1a7d2cbf44c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo Logistic Regression con características TF-IDF\n",
            "Accuracy (exactitud):  0.7604790419161677\n",
            "Modelo Naive Bayes con características TF-IDF\n",
            "Accuracy (exactitud):  0.7365269461077845\n",
            "Modelo Linear SVM con características TF-IDF\n",
            "Accuracy (exactitud):  0.7305389221556886\n"
          ]
        }
      ],
      "source": [
        "# SOLUCIÓN\n",
        "for m, clf in modelos:\n",
        "    print('Modelo {} con características TF-IDF'.format(m))\n",
        "    tfidf_predictions = train_predict_evaluate_model(classifier=clf,\n",
        "                                           train_features=tfidf_train_features,\n",
        "                                           train_labels=y_train,\n",
        "                                           test_features=tfidf_test_features,\n",
        "                                           test_labels=y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwlrDJRA5GYq"
      },
      "source": [
        "Vemos que la clasificación sí que ha mejorado.  \n",
        "Vemos cuáles son las características más importantes para el modelo LR sobre TF-IDF:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msPDaHPW5GYq",
        "outputId": "81917264-7e16-4428-cc3c-617edef4d2b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Menores Coefs:\n",
            "['no' 'porque' 'ni' 'triste' 'ese' 'poner' 'malo' 'sad' 'pobre' 'puto']\n",
            "\n",
            "Mayores Coefs: \n",
            "['buen' 'gran' 'gracia' 'genial' 'feliz' 'tranquilo' 'mejor' 'mucho'\n",
            " 'bien' 'guapo']\n"
          ]
        }
      ],
      "source": [
        "# obtenemos los nombres de las características numpy array\n",
        "feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Ordenamos los coeficientes del modelo\n",
        "sorted_coef_index = modelLR.coef_[0].argsort()\n",
        "\n",
        "# Listamos los 10 coeficientes menores y mayores\n",
        "print('Menores Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
        "print('Mayores Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfuKzHap5GYq"
      },
      "source": [
        "### EJERCICIO 4\n",
        "Haz un análisis de subjetividad de textos, para ello trata de capturar texto de diferentes páginas web y compara el nivel de subjetividad de cada uno de ellos. Investiga qué librerías existen en python para realizar dicho análisis. ¿Qué índices proporcionan estas librerías y en qué se basan?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaiyT_qTP1rh"
      },
      "source": [
        "--------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgViN4r_PUmo"
      },
      "source": [
        "Existen diversas librerías en Python que permiten realizar análisis de subjetividad de textos. Algunas de las más populares son:\n",
        "\n",
        "* NLTK (Natural Language Toolkit): una librería de procesamiento de lenguaje natural que incluye herramientas para realizar análisis de sentimiento y subjetividad. Ofrece modelos pre-entrenados para distintos idiomas y dominios, así como funciones para entrenar modelos propios.\n",
        "\n",
        "* TextBlob: una librería que ofrece una interfaz sencilla para realizar análisis de sentimiento y subjetividad. Incluye modelos pre-entrenados para inglés y permite entrenar modelos propios a partir de corpus etiquetados.\n",
        "\n",
        "* VaderSentiment: una librería especializada en análisis de sentimiento para redes sociales y otros textos informales. Utiliza un conjunto de reglas heurísticas para identificar expresiones positivas, negativas y neutrales.\n",
        "\n",
        "* Pattern: una librería que ofrece diversas funcionalidades para procesamiento de lenguaje natural, incluyendo análisis de sentimiento y subjetividad. Utiliza un modelo probabilístico basado en el análisis de frecuencia de palabras y expresiones.\n",
        "\n",
        "Cada una de estas librerías proporciona distintos índices y medidas para el análisis de subjetividad. Algunos de los más comunes son:\n",
        "\n",
        "* Polaridad: indica el grado de positividad o negatividad de un texto, en una escala que va de -1 a 1.\n",
        "\n",
        "* Subjetividad: indica el grado de subjetividad o objetividad de un texto, en una escala que va de 0 a 1.\n",
        "\n",
        "* Intensidad: indica el grado de intensidad emocional de un texto, en una escala que va de 0 a 1.\n",
        "\n",
        "Estos índices se basan en modelos estadísticos y algoritmos de aprendizaje automático que han sido entrenados con _corpus_ etiquetados. Cada librería utiliza distintas estrategias y enfoques para realizar el análisis de subjetividad, por lo que es importante evaluar su desempeño en distintos tipos de textos y contextos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-v5tGyaPTsX",
        "outputId": "4e73a42f-fe38-4e88-f595-289ce407771f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCEgWD_gQ3Oi"
      },
      "source": [
        "Vamos a analizar una misma noticia presentada en diferentes periódicos. Para ello, hemos de tener en cuenta los siguientes listados:\n",
        "\n",
        "_Periódicos de tendencia política de izquierda_:\n",
        "\n",
        "* El País\n",
        "* Público\n",
        "* La Vanguardia (centro-izquierda)\n",
        "* El Diario\n",
        "* Infolibre\n",
        "* El Salto\n",
        "\n",
        "_Periódicos de tendencia política de derecha_:\n",
        "\n",
        "* ABC\n",
        "* El Mundo\n",
        "* La Razón\n",
        "* Libertad Digital\n",
        "* Okdiario\n",
        "* El Español"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kj83hmJzScsh"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "K6p6dE7EPpr-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'neg': 0.0, 'neu': 0.99, 'pos': 0.01, 'compound': 0.5859}\n"
          ]
        }
      ],
      "source": [
        "# Noticia de El País\n",
        "url_1 = \"https://elpais.com/sociedad/2023-04-05/la-fiscalia-archiva-la-investigacion-por-los-canticos-machistas-del-elias-ahuja.html \"\n",
        "\n",
        "# Noticia Libertad_digital\n",
        "url_2 =\"https://www.libertaddigital.com/madrid/2023-04-05/la-fiscalia-archiva-la-investigacion-por-los-canticos-del-colegio-mayor-elias-ahuja-al-no-constituir-delito-de-odio-7002206/\"\n",
        "\n",
        "try:\n",
        "    page = requests.get(url_1)\n",
        "except:\n",
        "    print(\"Error al abrir la URL\")\n",
        "\n",
        "soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "# Buscamos el <div> correspondiente y sacamos su contenido:\n",
        "content_1 = soup.find('div', {\"class\": \"a_c clearfix\"})\n",
        "article_1 = []\n",
        "for i in content_1.find_all('p'):\n",
        "    article_1.append(i.text)\n",
        "\n",
        "article_1 = ''.join(article_1)\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')  # tokenizador para separar en palabras\n",
        "stop_words = set(stopwords.words('spanish'))  # lista de stopwords\n",
        "lemmatizer = WordNetLemmatizer()  # lematizador\n",
        "\n",
        "# Separar el texto en palabras\n",
        "words = tokenizer.tokenize(article_1.lower())\n",
        "\n",
        "# Filtrar stopwords y lematizar las palabras\n",
        "words = [lemmatizer.lemmatize(word, pos='v') for word in words if word not in stop_words]\n",
        "\n",
        "# Analizar la subjetividad del texto\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "polarity_scores = sia.polarity_scores(' '.join(words))\n",
        "\n",
        "print(polarity_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'neg': 0.0, 'neu': 0.992, 'pos': 0.008, 'compound': 0.2732}\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    page = requests.get(url_2)\n",
        "except:\n",
        "    print(\"Error al abrir la URL\")\n",
        "\n",
        "soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "# Buscamos el <div> correspondiente y sacamos su contenido:\n",
        "content_2 = soup.find('div', {\"class\": \"body\"})\n",
        "article_2 = []\n",
        "for i in content_2.find_all('p'):\n",
        "    article_2.append(i.text)\n",
        "\n",
        "article_2 = ''.join(article_2)\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')  # tokenizador para separar en palabras\n",
        "stop_words = set(stopwords.words('spanish'))  # lista de stopwords\n",
        "lemmatizer = WordNetLemmatizer()  # lematizador\n",
        "\n",
        "# Separar el texto en palabras\n",
        "words = tokenizer.tokenize(article_2.lower())\n",
        "\n",
        "# Filtrar stopwords y lematizar las palabras\n",
        "words = [lemmatizer.lemmatize(word, pos='v') for word in words if word not in stop_words]\n",
        "\n",
        "# Analizar la subjetividad del texto\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "polarity_scores = sia.polarity_scores(' '.join(words))\n",
        "\n",
        "print(polarity_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La Fiscalía archiva la investigación por los cánticos machistas del colegio mayor Elías Ahuja | Educación | EL PAÍSSeleccione:- - -EspañaAméricaMéxicoColombiaChileArgentinaUSAEducaciónsuscríbeteHHOLAIniciar sesiónEducaciónInfantil y PrimariaSecundaria, Bachillerato y FPUniversidadesÚltimas noticiasMACHISMOLa Fiscalía archiva la investigación por los cánticos machistas del colegio mayor Elías AhujaEl ministerio público considera que las expresiones fueron “irrespetuosas e insultantes para las mujeres”, pero no constituyen un delito de odio00:52\"Putas, salid de vuestras madrigueras como conejas\"Entrada al colegio mayor masculino Elías Ahuja, adscrito a la Universidad Complutense de Madrid.\n",
            "Foto: CLAUDIO ÁLVAREZ | Vídeo: EPVElisa SilióLucía BohórquezMadrid / Palma - 05 abr 2023 - 09:37Actualizado: 05 abr 2023 - 10:02 UTCWhatsappFacebookTwitterCopiar enlaceComentariosLa Fiscalía de Madrid ha acordado archivar las diligencias de investigación abiertas contra un alumno de la residencia de estudiantes Elías Ahuja de Madrid por los gritos sexistas lanzados la noche del 2 de octubre de 2022 a las residentes del colegio mayor femenino contiguo Santa Mónica, según ha informado este miércoles el ministerio público. Las diligencias se abrieron a raíz de una denuncia del Movimiento contra la Intolerancia al considerar que los hechos podrían ser constitutivos de un delito de odio.Se da la circunstancia de que la ley del solo sí es sí, que regula este tipo de conductas, entró en vigor el siguiente viernes, 7 de octubre. Esta norma castiga a quienes se dirijan a otra persona con expresiones, comportamientos o proposiciones de carácter sexual que creen en la víctima una situación objetivamente humillante, hostil o intimidatoria, sin llegar a constituir otros delitos de mayor gravedad.En febrero, el alumno que inició los cánticos que pronto se viralizaron ―“¡Putas, salid de vuestras madrigueras como conejas, sois unas putas ninfómanas, os prometo que vais a follar todas en la capea! ¡Vamos, Ahuja!”―, a los que se sumaron el resto de estudiantes simulando ser animales con sus ruidos, manifestó ante el fiscal que los gritos hacia sus vecinas del Santa Mónica ―muchas de ellas familiares― eran “una broma” que seguía “una tradición”, negando que su intención fuera humillar a las chicas. La escena, que se conoce como La Granja, se repetía cada año y las colegialas no se sintieron agredidas. “A mí si me llaman puta o ninfómana por la calle, claro que me ofendo, pero ellos son nuestros amigos”, subrayó una joven a este diario.El fiscal jefe no apreció tras la investigación “vinculación alguna con grupos o movimientos extremistas” previa o posterior del alumno que inició unos cánticos que tilda de “soeces y procaces”.  Y afirma en sus diligencias que el colegio mayor no había podido “identificar a los estudiantes que estaban en las instalaciones en ese momento debido a que el sistema de autorizaciones para salidas nocturnas es habilitado para todo el curso escolar”. Tampoco a quienes grabaron las imágenes que pronto se viralizaron.El decreto de archivo del fiscal sostiene que los hechos son “irrespetuosos e insultantes para las mujeres” y las expresiones proferidas constituyen “un ataque a la dignidad individual o colectiva de aquellas”. Sin embargo, no pueden ser por sí solas constitutivas de un delito de odio del artículo 510.2 del Código Penal, al exigir este delito la concurrencia de una motivación discriminatoria concreta, la cual no ha resultado acreditada en la investigación por hechos anteriores, coetáneos ni posteriores a los denunciados. Según la Fiscalía, la acción investigada no puede tipificarse tampoco como un delito contra la integridad moral porque para ello es necesario que alguna de las personas destinatarias de las expresiones proferidas se hubiera sentido ofendida y “no consta que ninguna de las mujeres que se encontraban en la residencia haya denunciado los hechos”.Aunque la conducta de los colegiales fue condenada por todo el arco político, la presidenta de la Comunidad de Madrid, Isabel Díaz Ayuso, se desmarcó entonces criticando a la Fiscalía: “A mí lo que me sorprende, sobre todo, es que la Fiscalía esté para investigar esto, mientras en la Universidad, a lo largo de los años, hemos visto en numerosas ocasiones pancartas a favor de los presos de ETA, hemos visto cómo han acosado y han montado escraches a profesores y alumnos impidiendo conferencias en libertad, o persiguen, por ejemplo, a los alumnos de S’ha acabat para que no puedan ir libremente a su universidad, a la facultad en Cataluña”.Cambios en la ley universitariaEl incidente machista del Ahuja no se ha reducido a objeto de debate en los medios durante días. A iniciativa de Más País, la Ley Orgánica del Sistema Universitario incluye un artículo que obliga a los colegios mayores ―supuestamente no son solo un alojamiento, sino que forman a los universitarios― a ser mixtos si no quieren ser expulsados de la red pública. Si siguen segregando, pasarán a ser residencias privadas, lo que les resta prestigio. Los colegios de titularidad pública hace tiempo que no separan, y en los privados como el Ahuja el proceso está siendo más largo.En los debates de las recientes elecciones a rector de la Complutense, como no podía ser de otra manera ―ha sido uno de sus últimos golpes reputacionales―, salió a colación el canon que los agustinos pagan a la universidad como contraprestación por tener el colegio en sus terrenos: 60.000 euros al año. Una cantidad pequeña si se tiene en cuenta que cada uno de los 174 colegiales paga 1.200 euros mensuales durante nueve meses (1,87 millones). Y esta cuenta no incluye las estancias en verano de otros huéspedes. “En los colegios mayores que antes no se pagaba, ahora se paga”, razonó el rector Joaquín Goyache en conversación con este diario. “Si vas al portal de transparencia, ves que hay residencias universitarias que pagan cerca de 500.000 euros y otros colegios mayores que rondan los 200.000 euros”, añadió la entonces candidata a rectora Esther del Campo.La justicia ha archivado el caso y el castigo también ha sido muy leve en el colegio para el estudiante de la Autónoma de Madrid que inició los cánticos. Aunque la dirección del centro había anunciado a los medios que su expulsión sería definitiva, luego se echó atrás. “El reglamento del centro por estos hechos prevé una expulsión de 15 días. El colegio lo amplió hasta 40 días y retiró al colegial una beca honorífica que tenía (...) Se ha puesto la sanción más alta que se ha podido según el reglamento”, aseguró a este diario. Sin embargo, el colegial, abrumado por su notoriedad, optó por alternar su estancia entre la residencia y casa de amigos en la capital.Puedes seguir EL PAÍS EDUCACIÓN en Facebook y Twitter, o apuntarte aquí para recibir nuestra newsletter semanal.ComentariosNormas Más informaciónEl difícil camino para que no se repitan los gritos del Elías Ahuja: “Sois unas pedazo de mierdas. ¡Putas!”Manuel Viejo / Elisa Silió | MadridLa nueva ley universitaria castigará a los colegios mayores que sigan segregando por sexos, como el Elías AhujaElisa Silió | MadridArchivado EnEducaciónMachismoSociedadMujeresHombresUniversidadSexismoFiscalíaComunidad de MadridMadridEspañaDelitos odioInjuriasResidencias de EstudiantesUCMColegios mayoresEstudiantesSe adhiere a los criterios deMás informaciónSi está interesado en licenciar este contenido contacte con ventacontenidos@prisamedia.comnewsletterRecibe el boletín de educaciónESPECIAL PUBLICIDADLa nueva Formación Profesional, una pasarela al empleoLo más vistoTrabajar en el ‘big data’: “Nunca he tenido que echar currículum, las ofertas me han llegado solas”El escándalo de los suspensos en el examen para dentistas extranjeros: “Es una burla, una estafa”Récord de ‘sisis’, más personas que nunca estudian y trabajan a la vez: “Es duro. Al final te pasa factura”Aprender animación y efectos especiales en Canarias con ‘Star Wars’ y ‘La casa del dragón’El Constitucional avala que se nieguen las subvenciones a los centros que segreguen por sexo Recomendaciones EL PAÍSDescuentosCursosCursos onlineEntradasInglés onlineEscaparateCrucigramas & Juegos ColeccionesDescuentosdescuentosUtiliza nuestro cupón AliExpress y ahórrate hasta un 50%descuentosAprovecha el código promocional El Corte Inglés y paga hasta un 50% menosdescuentosDisfruta del código promocional Amazon y consigue hasta 20% de descuentodescuentosCanjea el código descuento Groupon y paga un 20% menosCursoscursos¿Te gustaría especializarte en Adiestramiento y Estética animal? ¡Te ayudamos a encontrar los mejores cursos y formación profesional!cursosMBA 'online' con un 86% de descuento y acceso a bolsa de empleo. ¡Solicita más información!cursosEncuentra aquí los mejores cursos y formación profesional para especializarte en Dietética y Nutricióncursos¿Te gustaría especializarte en Cocina, Repostería y Enología? ¡Te ayudamos a encontrar los mejores cursos y formación profesional!Cursos onlinecursosonline¿Quieres especializarte en Odontología e Higiene Bucodental?  ¡Encuentra los mejores cursos y formación profesional aquí!cursosonlineEncuentra aquí los mejores cursos FP 'online' y a distancia en Instalaciones Eléctricas, Térmicas y de Gas cursosonline¿Quieres especializarte en Mecánica? ¡Encuentra los mejores cursos y formación profesional 'online' y a distancia aquí!cursosonlineMáster a distancia en Psicología Infantil y Juvenil con un 86% de descuento y acceso a bolsa de empleoEntradasentradas'WE WILL ROCK YOU'. Disfruta con los grandes éxitos de Queen. Hasta el 28 de mayo en Madridentradas'EL FANTASMA DE LA ÓPERA'. Llega a Madrid el musical más exitoso de la historiaentradasFestival 'RÍO BABEL 2023'. Con Morat, Julieta Venegas... Del 30 de junio al 2 de julio, Madridentradas'FITO PÁEZ' en Icónica Sevilla Fest 2023. Jueves 6 de julio. Entradas ya a la venta Inglés onlinecursosinglesMejora tu inglés con EL PAÍS con 15 minutos al díacursosinglesDisfruta de nuestras lecciones personalizadas, breves y divertidascursosinglesEvalúa tu nivel y obtén un certificadocursosinglesPrueba 21 días gratis y sin compromisoEscaparateescaparateUn maletín de llaves y destornillador superventas por menos de 30€escaparate12 cremas milagrosas para curar cicatricesescaparateProbamos los mejores amplificadores wifi de enchufeescaparateLa luz trasera para bici que arrasa en AmazonCrucigramas & Juegos juegosCrucigramas para expertosjuegosCrucigramas minisjuegosCrucigramas de Tarkus. juegosSopas de letras temáticas JurjoColeccionescoleccionesLos mejores libros de la literatura universal recopilados en una colección únicacolecciones'El hombre invisible', 'Otra vuelta de tuerca' y 'El corazón de las tinieblas' entre otroscoleccionesAprovecha esta oportunidadcoleccionesY hazte con la colección completaRecomendaciones EL PAÍSCursosCentrosFrancés onlineInglés onlineItaliano onlineAlemán onlineCrucigramas & Juegos CursoscursosMaestría en Big Data y Analytics 100% en líneacursosMBA Administración y Dirección de Empresas en líneacursosPrograma en línea en 'Project Finance' InternacionalcursosMaestría en línea en Dirección de Recursos Humanos y Gestión del TalentoCentroscursosonlineMaestría en línea en Gestión AmbientalcursosonlineMaestría en Comercio Internacional presencial en Madrid, EspañacursosonlineMaestría en Marketing Digital & E-Commerce. Semipresencial en Madrid, EspañacursosonlineMaestría en 'Data Management' e Innovación Tecnológica 100% en líneaFrancés onlinecursosinglesMejora tu francés con 15 minutos al díacursosinglesDisfruta de nuestras lecciones personalizadas, breves y divertidas.cursosinglesObtendrá un diploma con estadísticas de nivel, progresión y participación. cursosingles 21 días de prueba gratuita de nuestro curso de francés ‘online’Inglés onlinecursosinglesMejore su inglés con EL PAÍS con 15 minutos al díacursosinglesDisfrute de nuestras lecciones personalizadas, breves y divertidascursosinglesEvalúe su nivel y obtenga un certificadocursosinglesPruebe 21 días gratis y sin compromisoItaliano onlinecursosinglesMejore su italiano con EL PAÍS con 15 minutos al díacursosinglesDisfrute de nuestras lecciones personalizadas, breves y divertidascursosinglesEvalúe su nivel y obtenga un certificadocursosinglesPruebe 21 días gratis y sin compromisoAlemán onlinecursosinglesLas mejores oportunidades hablan alemán. Nuevo curso 'online' cursosinglesDisfrute de nuestras lecciones personalizadas, breves y divertidascursosinglesEvalúe su nivel y obtenga un certificadocursosinglesPruebe 21 días gratis y sin compromisoCrucigramas & Juegos juegosCrucigramas minisjuegosCrucigramas TarkusjuegosSudokus minijuegosSopas de letras  Recomendaciones EL PAÍSCursosCentros Italiano onlineFrancés onlineAlemán onlineCrucigramas & Juegos CursoscursosMaestría en Ciencias Ambientales presencial en Benito JuárezcursosLicenciatura en Administración de Empresas presencial en Benito JuárezcursosMaestría a distancia en Lingüística Aplicada a la Enseñanza del Español como Lengua ExtranjeracursosLicenciatura Ejecutiva en Psicología Semipresencial. Cinco sedes disponiblesCentros cursosonlineLicenciatura Ejecutiva en Derecho. Semipresencial en AguascalientescursosonlineMaestría a distancia en Actividad Física y SaludcursosonlineMaestría a distancia en Energías RenovablescursosonlineDescubre un completo Directorio de Centros de FormaciónItaliano onlinecursosinglesMejore su italiano con solo 15 minutos al díacursosinglesDisfrute de nuestras lecciones personalizadas, breves y divertidas.cursosinglesObtendrá un diploma con estadísticas de nivel, progresión y participación.cursosingles21 días de prueba gratuita. ¡Empiece ya!Francés onlinecursosinglesMejore su francés con solo 15 minutos al díacursosinglesDisfrute de nuestras lecciones personalizadas, breves y divertidas.cursosinglesObtendrá un diploma con estadísticas de nivel, progresión y participación.cursosingles21 días de prueba gratuita. ¡Empiece ya!Alemán onlinecursosinglesLas mejores oportunidades hablan alemán. Nuevo curso 'online' cursosinglesDisfrute de nuestras lecciones personalizadas, breves y divertidas.cursosinglesObtendrá un diploma con estadísticas de nivel, progresión y participación.cursosingles21 días de prueba gratuita. ¡Empiece ya!Crucigramas & Juegos juegos¡Disfruta con nuestros Crucigramas para expertos!juegosResuelve los últimos Crucigramas de MambrinojuegosJuega a nuestros Sudoku para Expertos y mejora día a día tu niveljuegosJuega a las nuevas Sopas de letras clásicas y temáticas de EL PAÍSRegístrate gratis para seguir leyendoINICIA SESIÓNCREAR CUENTAO suscríbete para leer sin límitesInicia sesión o regístrate gratis para continuar leyendo en incógnitoRegístrate gratisInicia sesiónSuscríbete y lee sin límitesVer opciones de suscripciónHOLAsuscríbete por 1€Mi actividadMi suscripciónMis datosMis newslettersDerechos y bajaExperiencias para mídesconectaralto contraste:BuscarSeleccione:- - -EspañaAméricaMéxicoColombiaChileArgentinaUSASi quieres seguir toda la actualidad sin límites, únete a EL PAÍS por 1€ el primer mesSUSCRÍBETE AHORAInternacionalOpiniónEditorialesTribunasViñetasCartas a la DirectoraDefensora del lectorEspañaAndalucíaCataluñaComunidad ValencianaGaliciaMadridPaís VascoEconomíaMercadosViviendaMis derechosFormaciónSociedadEducaciónClima y Medio AmbienteCienciaSaludTecnologíaCulturaDeportesTelevisiónGente y Estilo de vidaFotografíaVídeosPodcastsel país semanalideasNegociosBabeliaQuadernel viajeroplaneta futuros modaiconGastroel comidistacinco díasMotorMamas & PapasAmérica FuturaEscaparateCrucigramas y JuegosNewsletterÚltimas noticiasHemerotecaEspecialesServiciosDescuentosColeccionesSíguenos en: Comentarios NormasSuscríbete en El País para participarYa tengo una suscripción\n",
            "{'neg': 0.026, 'neu': 0.961, 'pos': 0.014, 'compound': -0.7269}\n",
            "['fiscal', 'madr', 'acord', 'archiv', 'diligent', 'investig', 'abiert', 'alumn', 'resident', 'estudi', 'eli', 'ahuj', 'madr', 'grit', 'sexist', 'lanz', 'noch', '2', 'octubr', '2022', 'resident', 'colegi', 'mayor', 'femenin', 'contigu', 'sant', 'monic', 'segun', 'inform', 'miercol', 'ministeri', 'public', 'diligent', 'abrieron', 'raiz', 'denunci', 'movimient', 'intoler', 'consider', 'hech', 'podr', 'ser', 'constitut', 'delit', 'odi', 'da', 'circunst', 'ley', 'sol', 'regul', 'tip', 'conduct', 'entro', 'vigor', 'siguient', 'viern', '7', 'octubr', 'norm', 'castig', 'dirij', 'person', 'expresion', 'comport', 'proposicion', 'caract', 'sexual', 'cre', 'victim', 'situacion', 'objet', 'humill', 'hostil', 'intimidatori', 'lleg', 'constitu', 'delit', 'mayor', 'graved', 'febrer', 'alumn', 'inic', 'cantic', 'pront', 'viraliz', 'put', 'sal', 'madriguer', 'conej', 'unas', 'put', 'ninfoman', 'promet', 'vais', 'foll', 'tod', 'cape', 'vam', 'ahuj', 'sum', 'rest', 'estudi', 'simul', 'ser', 'animal', 'ruid', 'manifest', 'fiscal', 'grit', 'haci', 'vecin', 'sant', 'monic', 'much', 'familiar', 'brom', 'segu', 'tradicion', 'neg', 'intencion', 'humill', 'chic', 'escen', 'conoc', 'granj', 'repet', 'cad', 'año', 'colegial', 'sint', 'agred', 'si', 'llam', 'put', 'ninfoman', 'call', 'clar', 'ofend', 'amig', 'subray', 'jov', 'diari', 'fiscal', 'jef', 'aprec', 'tras', 'investig', 'vincul', 'algun', 'grup', 'movimient', 'extrem', 'previ', 'posterior', 'alumn', 'inic', 'cantic', 'tild', 'soec', 'procac', 'afirm', 'diligent', 'colegi', 'mayor', 'pod', 'identific', 'estudi', 'instal', 'moment', 'deb', 'sistem', 'autoriz', 'sal', 'nocturn', 'habilit', 'curs', 'escol', 'tampoc', 'grab', 'imagen', 'pront', 'viraliz', 'decret', 'archiv', 'fiscal', 'sostien', 'hech', 'irrespetu', 'insult', 'mujer', 'expresion', 'profer', 'constitu', 'ataqu', 'dignid', 'individual', 'colect', 'aquell', 'embarg', 'pued', 'ser', 'sol', 'constitut', 'delit', 'odi', 'articul', '510', '2', 'codig', 'penal', 'exig', 'delit', 'concurrent', 'motiv', 'discriminatori', 'concret', 'result', 'acredit', 'investig', 'hech', 'anterior', 'coetane', 'posterior', 'denunci', 'segun', 'fiscal', 'accion', 'investig', 'pued', 'tipific', 'tampoc', 'delit', 'integr', 'moral', 'ello', 'necesari', 'algun', 'person', 'destinatari', 'expresion', 'profer', 'ofend', 'const', 'ningun', 'mujer', 'encontr', 'resident', 'denunci', 'hech', 'aunqu', 'conduct', 'colegial', 'conden', 'arco', 'polit', 'president', 'comun', 'madr', 'isabel', 'diaz', 'ayus', 'desmarc', 'entonc', 'critic', 'fiscal', 'sorprend', 'fiscal', 'investig', 'mientr', 'univers', 'larg', 'años', 'vist', 'numer', 'ocasion', 'pancart', 'favor', 'pres', 'eta', 'vist', 'com', 'acos', 'mont', 'escrach', 'profesor', 'alumn', 'impid', 'conferent', 'libert', 'persig', 'ejempl', 'alumn', 's', 'acabat', 'pued', 'ir', 'librement', 'univers', 'facult', 'cataluñ', 'incident', 'machist', 'ahuj', 'reduc', 'objet', 'debat', 'medi', 'dias', 'inici', 'pais', 'ley', 'organ', 'sistem', 'universitari', 'inclu', 'articul', 'oblig', 'colegi', 'mayor', 'supuest', 'sol', 'aloj', 'sin', 'form', 'universitari', 'ser', 'mixt', 'si', 'quier', 'ser', 'expuls', 'red', 'public', 'si', 'sig', 'segreg', 'pas', 'ser', 'resident', 'priv', 'rest', 'prestigi', 'colegi', 'titular', 'public', 'hac', 'tiemp', 'sep', 'priv', 'ahuj', 'proces', 'siend', 'larg', 'debat', 'recient', 'eleccion', 'rector', 'complutens', 'pod', 'ser', 'maner', 'sid', 'ultim', 'golp', 'reputacional', 'sal', 'colacion', 'canon', 'agustin', 'pag', 'univers', 'contraprest', 'ten', 'colegi', 'terren', '60', '000', 'eur', 'año', 'cantid', 'pequeñ', 'si', 'cuent', 'cad', '174', 'colegial', 'pag', '1', '200', 'eur', 'mensual', 'nuev', 'mes', '1', '87', 'millon', 'cuent', 'inclu', 'estanci', 'veran', 'huesped', 'colegi', 'mayor', 'pag', 'ahor', 'pag', 'razon', 'rector', 'joaquin', 'goyach', 'convers', 'diari', 'si', 'vas', 'portal', 'transparent', 'ves', 'resident', 'universitari', 'pag', 'cerc', '500', '000', 'eur', 'colegi', 'mayor', 'rond', '200', '000', 'eur', 'añad', 'entonc', 'candidat', 'rector', 'esther', 'camp', 'justici', 'archiv', 'cas', 'castig', 'sid', 'lev', 'colegi', 'estudi', 'autonom', 'madr', 'inic', 'cantic', 'aunqu', 'direccion', 'centr', 'anunci', 'medi', 'expulsion', 'definit', 'lueg', 'echo', 'atras', 'reglament', 'centr', 'hech', 'prev', 'expulsion', '15', 'dias', 'colegi', 'ampli', '40', 'dias', 'retir', 'colegial', 'bec', 'honorif', 'puest', 'sancion', 'alta', 'pod', 'segun', 'reglament', 'asegur', 'diari', 'embarg', 'colegial', 'abrum', 'notoried', 'opto', 'altern', 'estanci', 'resident', 'cas', 'amig', 'capital', 'pued', 'segu', 'pais', 'educ', 'facebook', 'twitt', 'apuntart', 'aqu', 'recib', 'newslett', 'semanal']\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "url = \"https://elpais.com/sociedad/2023-04-05/la-fiscalia-archiva-la-investigacion-por-los-canticos-machistas-del-elias-ahuja.html\"\n",
        "\n",
        "try:\n",
        "    page = requests.get(url)\n",
        "except:\n",
        "    print(\"Error al abrir la URL\")\n",
        "\n",
        "soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "\n",
        "# Buscamos el <div> correspondiente y sacamos su contenido:\n",
        "content = soup.find('div', {\"class\": \"a_c clearfix\"})\n",
        "article = []\n",
        "for i in content.find_all('p'):\n",
        "    article.append(i.text)\n",
        "\n",
        "article = ''.join(article)\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')  # tokenizador para separar en palabras\n",
        "stop_words = set(stopwords.words('spanish'))  # lista de stopwords\n",
        "stemmer = SnowballStemmer('spanish')  # lematizador para español\n",
        "\n",
        "# Separar el texto en palabras\n",
        "words = tokenizer.tokenize(article.lower())\n",
        "\n",
        "# Filtrar stopwords y lematizar las palabras\n",
        "words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "\n",
        "# Analizar la subjetividad del texto\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "polarity_scores = sia.polarity_scores(' '.join(words))\n",
        "\n",
        "print(polarity_scores)\n",
        "print(words)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
