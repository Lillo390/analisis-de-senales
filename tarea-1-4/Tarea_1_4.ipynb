{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYLGwJRK5GYe"
      },
      "source": [
        "# An√°lisis de sentimientos con NLP\n",
        "Vamos a utilizar Spacy y scikit-learn para clasificar con conjunto de tweets en espa√±ol como positivos/negativos\n",
        "\n",
        "## Carga y preparaci√≥n de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "PQbQKKtV5GYg",
        "outputId": "1b28c201-b95e-4cab-b913-3ccbd2ed1ee0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-Me caes muy bien \\r\\n-Tienes que jugar m√°s partidas al lol con Russel y conmigo\\r\\n-Por qu√© tan Otako, deja de ser otako\\r\\n-Haber si me muero</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@myendlesshazza a. que puto mal escribo\\r\\n\\r\\nb. me sigo surrando help \\r\\n\\r\\n3. ha quedado raro el \"c√≥metelo\" ah√≠ JAJAJAJA</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@estherct209 jajajaja la tuya y la d mucha gente seguro!! Pero yo no puedo sin mi melena me muero</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Quiero mogoll√≥n a @AlbaBenito99 pero sobretodo por lo r√°pido que contesta a los wasaps</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Vale he visto la tia bebiendose su regla y me hs dado muchs grima</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                           content  \\\n",
              "0  -Me caes muy bien \\r\\n-Tienes que jugar m√°s partidas al lol con Russel y conmigo\\r\\n-Por qu√© tan Otako, deja de ser otako\\r\\n-Haber si me muero   \n",
              "1                    @myendlesshazza a. que puto mal escribo\\r\\n\\r\\nb. me sigo surrando help \\r\\n\\r\\n3. ha quedado raro el \"c√≥metelo\" ah√≠ JAJAJAJA   \n",
              "2                                               @estherct209 jajajaja la tuya y la d mucha gente seguro!! Pero yo no puedo sin mi melena me muero    \n",
              "3                                                          Quiero mogoll√≥n a @AlbaBenito99 pero sobretodo por lo r√°pido que contesta a los wasaps    \n",
              "4                                                                               Vale he visto la tia bebiendose su regla y me hs dado muchs grima    \n",
              "\n",
              "  polarity  \n",
              "0     NONE  \n",
              "1        N  \n",
              "2        N  \n",
              "3        P  \n",
              "4        N  "
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "pd.set_option('display.max_colwidth', None) # leer m√°ximo ancho de columna\n",
        "\n",
        "# Leemos los datos\n",
        "data_path ='./tweets_all.csv'\n",
        "df = pd.read_csv(data_path, index_col=None)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThihT6_S5GYh",
        "outputId": "a9a7bdb6-d723-4766-c1c2-74c54a618700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1514 entries, 0 to 1513\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   content   1514 non-null   object\n",
            " 1   polarity  1514 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 23.8+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMWmyoh05GYh",
        "outputId": "76d4fa8c-1266-47b8-cd1c-c514be3997d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "N       637\n",
              "P       474\n",
              "NEU     202\n",
              "NONE    201\n",
              "Name: polarity, dtype: int64"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.polarity.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrGqiLLO5GYh"
      },
      "source": [
        "Tenemos 1514 tweets, de los cuales hay 637 positivos y 474 negativos. El resto son neutros o no tienen polaridad clasificada.\n",
        "Vamos a entrenar s√≥lo con los positivos y negativos para utilizar un clasificador binario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zUANnBFN5GYi"
      },
      "outputs": [],
      "source": [
        "df = df[(df['polarity']=='P') | (df['polarity']=='N')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWFmZqrY5GYi",
        "outputId": "c1d2a6fd-17c3-4621-a13a-83b5041cfddf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "N    637\n",
              "P    474\n",
              "Name: polarity, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.polarity.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (3.5.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: setuptools in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (61.2.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
            "Collecting es-core-news-md==3.3.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.3.0/es_core_news_md-3.3.0-py3-none-any.whl (42.3 MB)\n",
            "Collecting spacy<3.4.0,>=3.3.0.dev0\n",
            "  Using cached spacy-3.3.2-cp39-cp39-win_amd64.whl (11.7 MB)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (0.4.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (3.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.4.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (61.2.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (21.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.27.1)\n",
            "Collecting thinc<8.1.0,>=8.0.14\n",
            "  Using cached thinc-8.0.17-cp39-cp39-win_amd64.whl (1.0 MB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (6.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (1.0.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.0.8)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (0.7.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (1.21.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.0.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (8.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\danie\\anaconda3\\envs\\analitica_web\\lib\\site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->es-core-news-md==3.3.0) (2.0.1)\n",
            "Installing collected packages: thinc, spacy, es-core-news-md\n",
            "Successfully installed es-core-news-md-3.3.0 spacy-3.3.2 thinc-8.0.17\n",
            "‚úî Download and installation successful\n",
            "You can now load the package via spacy.load('es_core_news_md')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script spacy.exe is installed in 'C:\\Users\\danie\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
          ]
        }
      ],
      "source": [
        "!pip install -U --user spacy\n",
        "!python -m spacy download es_core_news_md --user\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc0g2x_X5GYi"
      },
      "source": [
        "## Limpieza de texto\n",
        "Hacemos un peque√±o pre-procesado del texto antes de extraer las caracter√≠sticas:  \n",
        "- Quitamos las menciones y las URL del texto porque no aportan valor para el an√°lisis de sentimientos.\n",
        "- Los hashtag s√≠ que pueden aportar valor as√≠ que simplemente quitamos el #.\n",
        "- Quitamos los signos de puntuaci√≥n y palabras menores de 3 caracteres.\n",
        "- Por √∫ltimo quitamos todos los s√≠mbolos de puntuaci√≥n del texto (que forman parte de un token).\n",
        "- Lematizamos el texto y lo guardamos en otra columna para comparar resultados del clasificador. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SCU2fEuR5GYi"
      },
      "outputs": [],
      "source": [
        "# Debes instalar las librer√≠as necesarias\n",
        "import re, string, spacy\n",
        "nlp=spacy.load('es_core_news_md')  # carga el modelo en espa√±ol es_core_news_md de la librer√≠a de spacy para hacer NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fCSK_orC5GYi"
      },
      "outputs": [],
      "source": [
        "# Lista de stop-words espec√≠ficos de nuestro corpus (aproximaci√≥n). Nota*: El corpus es el conjunto de textos que sirven como base para el an√°lisis ling√º√≠stico\n",
        "stop_words = ['el', 'la', 'lo', 'los', 'las', 'un', 'una', 'unos', 'unas', 'me', 'a', 'de', 'se', 'te']\n",
        "\n",
        "pattern2 = re.compile('[{}]'.format(re.escape(string.punctuation))) #selecciona s√≠mbolos de puntuaci√≥n\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Limpiamos las menciones y URL del texto. Luego convertimos todo en tokens,\n",
        "    eliminamos los tokens que son signos de puntuaci√≥n y convertimos en\n",
        "    min√∫sculasn. Para terminar, volvemos a convertir en cadena de texto\"\"\"\n",
        "\n",
        "    text = re.sub(r'@[\\w_]+|https?://[\\w_./]+', '', text) #elimina menciones y URL\n",
        "    tokens = nlp(text)\n",
        "    tokens = [tok.lower_ for tok in tokens if not tok.is_punct and not tok.is_space]\n",
        "    filtered_tokens = [pattern2.sub('', token) for token in tokens if not (token in stop_words)] #obvia stop_words y despu√©s quita signos de puntuaci√≥n\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    \n",
        "    return filtered_text\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    \"\"\"Convertimos el texto a tokens, extraemos el lexema de cada token\n",
        "    y volvemos a convertir en cadena de texto\"\"\"\n",
        "    tokens = nlp(text)\n",
        "    lemmatized_tokens = [tok.lemma_ for tok in tokens]\n",
        "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "    \n",
        "    return lemmatized_text\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B_aAaRz5GYj"
      },
      "source": [
        "Probamos el funcionamiento de estas funciones sobre un tweet de ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4Qj8hOs5GYj",
        "outputId": "ecd4eb40-250a-43d8-ad8d-0ad91e0be212"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:\n",
            " Mg y pongo un adjetivo super repelente a vuestro nombre \n",
            "\n",
            "Limpiado:\n",
            " mg y pongo adjetivo super repelente vuestro nombre\n",
            "\n",
            "Lematizado:\n",
            " mg y poner adjetivo super repelente vuestro nombre\n"
          ]
        }
      ],
      "source": [
        "print('Original:\\n',df.content[10])\n",
        "print('\\nLimpiado:\\n',clean_text(df.content[10]))\n",
        "print('\\nLematizado:\\n',lemmatize_text(clean_text(df.content[10])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSNgd86O5GYj"
      },
      "source": [
        "Aplicamos limpieza a todos los tweets del DataFrame y creamos columna nueva con los lemas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "syK0P5hZ5GYj"
      },
      "outputs": [],
      "source": [
        "df.content=df.content.apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oFrFlRJ15GYk"
      },
      "outputs": [],
      "source": [
        "#Quitamos tweets vac√≠os despu√©s de la limpieza\n",
        "df=df[df.content!='']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZW4fvE3l5GYk"
      },
      "outputs": [],
      "source": [
        "df[\"lemas\"]=df.content.apply(lemmatize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MCzHyWyu5GYk"
      },
      "outputs": [],
      "source": [
        "#Contamos el n¬∫ de palabras por tweet\n",
        "df['content_words'] = [len(t.split(' ')) for t in df.content]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNEvewfV5GYk",
        "outputId": "c41d9502-d7f6-4617-b322-1afdd16b954d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       content_words\n",
            "count    1111.000000\n",
            "mean       12.166517\n",
            "std         4.764943\n",
            "min         3.000000\n",
            "25%         8.000000\n",
            "50%        12.000000\n",
            "75%        16.000000\n",
            "max        26.000000\n",
            "                                                                               content  \\\n",
            "1  a que puto mal escribo b sigo surrando help 3 ha quedado raro c√≥metelo ah√≠ jajajaja   \n",
            "2            jajajaja tuya y d mucha gente seguro pero yo no puedo sin mi melena muero   \n",
            "3                        quiero mogoll√≥n pero sobretodo por r√°pido que contesta wasaps   \n",
            "4                          vale he visto tia bebiendose su regla y hs dado muchs grima   \n",
            "5                      ah mucho m√°s por supuesto solo que incluyo hab√≠as entendido mal   \n",
            "\n",
            "  polarity  \\\n",
            "1        N   \n",
            "2        N   \n",
            "3        P   \n",
            "4        N   \n",
            "5        P   \n",
            "\n",
            "                                                                                     lemas  \\\n",
            "1  a que puto mal escribir b seguir surrar help 3 haber quedar raro c√≥metelo ah√≠ jajajajar   \n",
            "2               jajajajar tuya y d mucho gente seguro pero yo no poder sin mi melena muero   \n",
            "3                          querer mogoll√≥n pero sobretodir por r√°pido que contestar wasaps   \n",
            "4                              valer haber ver tia beber √©l su regla y hs dado muchs grima   \n",
            "5                            ah mucho m√°s por supuesto solo que incluyo haber entender mal   \n",
            "\n",
            "   content_words  \n",
            "1             16  \n",
            "2             15  \n",
            "3              9  \n",
            "4             12  \n",
            "5             11  \n"
          ]
        }
      ],
      "source": [
        "print(df.describe())\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGtOtXcV5GYk"
      },
      "source": [
        "Vemos que de media cada tweet tiene unas 12 palabras, con un m√°ximo de 26 palabras.  \n",
        "### Clasificador\n",
        "Vamos a usar la librer√≠a scikit-learn para aplicar un clasificador binario sobre la polaridad. Aplicamos dos modelos distintos para extraer las caracter√≠sticas del texto, Bag-of-Words (BoW) y Term Frequency times Inverse Document Frequency (TF-IDF).  \n",
        "\n",
        "Primero dividimos en conjunto de entrenamiento y test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "b9VOZOkB5GYk"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and test sets\n",
        "# Asignamos un 70% a training y un 30% a test\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['content'], \n",
        "                                                    df['polarity'],\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F7IrfTZ5GYl",
        "outputId": "6f43bc62-ab71-41d8-a3c9-9f351f1fa998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Primera entrada de train:\n",
            " estoy preparando muchas cosas para que especial sea muy especial\n",
            "Polaridad: P\n",
            "\n",
            "X_train shape: (777,)\n",
            "\n",
            "X_test shape: (334,)\n"
          ]
        }
      ],
      "source": [
        "print('Primera entrada de train:\\n', X_train.iloc[0])\n",
        "print('Polaridad:', y_train.iloc[0])\n",
        "print('\\nX_train shape:', X_train.shape)\n",
        "print('\\nX_test shape:', X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swqSr2q-5GYl"
      },
      "source": [
        "## Modelo Bag of Words\n",
        "El BoW se implementa con la funci√≥n `CountVectorizer` de `scikit-learn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGOpGiG25GYl"
      },
      "source": [
        "El objetivo de la bolsa de palabras es transformar el texto en una representaci√≥n num√©rica que se pueda utilizar en algoritmos de aprendizaje autom√°tico. El proceso implica los siguientes pasos:\n",
        "\n",
        "Tokenizaci√≥n: El texto se divide en tokens o palabras individuales.\n",
        "\n",
        "Limpieza: Se eliminan las palabras vac√≠as (stopwords) y se realizan otras operaciones de limpieza del texto como la lematizaci√≥n.\n",
        "\n",
        "Creaci√≥n del vocabulario: Se crea un vocabulario de todas las palabras √∫nicas que aparecen en el conjunto de documentos.\n",
        "\n",
        "Conteo de frecuencia: Se cuenta el n√∫mero de veces que cada palabra aparece en cada documento.\n",
        "\n",
        "Creaci√≥n de la matriz de caracter√≠sticas: Se crea una matriz que representa cada documento como un vector de caracter√≠sticas, donde cada caracter√≠stica corresponde a una palabra del vocabulario y su valor es el n√∫mero de veces que esa palabra aparece en el documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "hUyqgAjb5GYl",
        "outputId": "2a72fed1-e5e6-4e72-e7fe-3e67458bdbee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CountVectorizer()"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# aprendemos el modelo CountVectorizer sobre el conjunto de train\n",
        "vect = CountVectorizer()\n",
        "vect.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPKbK4Tn5GYl"
      },
      "source": [
        "Vemos el n√∫mero de t√©rminos distintos que tiene el diccionario:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTHZoHy55GYl",
        "outputId": "9e4b185d-b465-4861-bb2b-e0c110fcc636"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3277"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vect.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnaWdsEO5GYl"
      },
      "source": [
        "Creamos la matriz BoW del conjunto de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO6BzoPd5GYl",
        "outputId": "381cec46-0937-4728-b236-01478e2bacd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(777, 3277)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# transformamos documentos de train en matriz de caracter√≠sticas\n",
        "X_train_vectorized = vect.transform(X_train)\n",
        "\n",
        "train=X_train_vectorized.toarray()\n",
        "print(train.shape)\n",
        "sum(train[:][0])  # hay diez elementos encontrados para la caracter√≠stica (feature) primera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOZGOAn25GYl"
      },
      "source": [
        "### Entrenamiento del modelo\n",
        "Vamos a probar un clasificador Logistic Regression de scikit-learn para entrenar nuestro modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "4CqGYdhc5GYm",
        "outputId": "e7780f02-cd65-49df-8628-ffc11eeb1699"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LogisticRegression(solver='liblinear')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "modelLR = LogisticRegression(solver='liblinear')\n",
        "#Entrenamos el modelo con el conjunto de train\n",
        "modelLR.fit(X_train_vectorized, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ2TPgCl5GYm"
      },
      "source": [
        "### Verificaci√≥n del modelo\n",
        "Para ver el rendimiento del modelo usamos el conjunto de test. Primero transformamos el conjunto de test a su matriz BoW mediante el vectorizador aprendido en TRAIN y aplicamos el modelo entrenado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et7t3fbi5GYm",
        "outputId": "6d21ad38-09aa-48c9-ac62-790f3a63754b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(334, 3277)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Predecimos sobre el conjunto de test\n",
        "X_test_vectorized = vect.transform(X_test)\n",
        "X_test_vectorized.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py659bRR5GYm",
        "outputId": "882f8cfc-8be3-424d-80fa-9a80abebff1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['N' 'N' 'N' 'P' 'N' 'N' 'P' 'N' 'P' 'N' 'N' 'P' 'P' 'P' 'N' 'N' 'N' 'P'\n",
            " 'P' 'P' 'N' 'P' 'P' 'P' 'N' 'N' 'N' 'P' 'P' 'N' 'P' 'P' 'P' 'P' 'N' 'N'\n",
            " 'N' 'N' 'N' 'P' 'P' 'N' 'N' 'N' 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'P' 'P' 'P'\n",
            " 'P' 'P' 'N' 'P' 'N' 'N' 'P' 'N' 'P' 'P' 'N' 'N' 'P' 'N' 'P' 'N' 'P' 'P'\n",
            " 'P' 'N' 'N' 'N' 'P' 'P' 'N' 'N' 'N' 'P' 'P' 'P' 'N' 'N' 'N' 'P' 'N' 'N'\n",
            " 'N' 'N' 'N' 'N' 'N' 'P' 'N' 'P' 'N' 'P' 'N' 'N' 'P' 'N' 'N' 'N' 'P' 'N'\n",
            " 'P' 'N' 'P' 'P' 'N' 'N' 'N' 'P' 'P' 'N' 'N' 'N' 'N' 'N' 'P' 'N' 'P' 'N'\n",
            " 'N' 'N' 'P' 'N' 'P' 'N' 'P' 'N' 'N' 'P' 'N' 'P' 'N' 'P' 'N' 'N' 'N' 'N'\n",
            " 'P' 'N' 'P' 'P' 'P' 'P' 'P' 'N' 'P' 'N' 'N' 'P' 'P' 'P' 'P' 'N' 'N' 'N'\n",
            " 'P' 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'P' 'P' 'N' 'P' 'N' 'P' 'N' 'N' 'P' 'P'\n",
            " 'P' 'P' 'P' 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'P'\n",
            " 'P' 'P' 'N' 'N' 'N' 'P' 'N' 'N' 'N' 'P' 'P' 'P' 'N' 'N' 'N' 'N' 'N' 'N'\n",
            " 'N' 'P' 'P' 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'N' 'P' 'N' 'P' 'N' 'N' 'P' 'N'\n",
            " 'N' 'P' 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'P' 'P' 'P' 'N' 'P' 'N' 'N' 'N' 'P'\n",
            " 'N' 'N' 'N' 'P' 'P' 'P' 'P' 'N' 'N' 'P' 'N' 'N' 'N' 'P' 'N' 'P' 'N' 'N'\n",
            " 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'P' 'N' 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'P' 'N'\n",
            " 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'P' 'P' 'N' 'N' 'P' 'N' 'N' 'P' 'P' 'N'\n",
            " 'P' 'N' 'P' 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N'\n",
            " 'N' 'N' 'N' 'P' 'P' 'P' 'N' 'N' 'N' 'P']\n"
          ]
        }
      ],
      "source": [
        "prediccion = modelLR.predict(X_test_vectorized)\n",
        "print(prediccion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE409oAC5GYm"
      },
      "source": [
        "Vemos el resultado de la predicci√≥n y calculamos su precisi√≥n con distintas m√©tricas.  \n",
        "Ejemplo de predicci√≥n de algunas muestras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "53EwpQMK5GYm",
        "outputId": "1b76c675-2a02-4eed-b21b-18d2bdda13ce"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>texto</th>\n",
              "      <th>polaridad</th>\n",
              "      <th>predicci√≥n</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>totalmente pobre gappy que inocente es no sabe con quien ha encontrado</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1246</th>\n",
              "      <td>ha explotado vaso en mano que forma tan bonita empezar d√≠a</td>\n",
              "      <td>N</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1033</th>\n",
              "      <td>que kinox no quiere zi yo zoy buena perzona</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1191</th>\n",
              "      <td>isco con juande no seria titular y sabeis al mister le van mas tissones</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>he capturado mi primer gimnasio en pokemon go cierto es que es probable que no tarden ni 10 minutos en quitarmelo pero mola</td>\n",
              "      <td>P</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1373</th>\n",
              "      <td>mi padre le ha dado manotazo mi m√≥vil y gracias dios que que ha roto ha sido cristal templado</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>ojal√° justin subiendo foto comi√©ndose boca con sof√≠a para que os den por culo todas</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>645</th>\n",
              "      <td>tiene que ser entretenido tu seras primera en verlo amore mio üòò</td>\n",
              "      <td>P</td>\n",
              "      <td>P</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1261</th>\n",
              "      <td>yo puedo cambiar opini√≥n cara es m√°s dif√≠cil porque vale pasta</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>941</th>\n",
              "      <td>y que sea yo es triste</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                            texto  \\\n",
              "134                                                        totalmente pobre gappy que inocente es no sabe con quien ha encontrado   \n",
              "1246                                                                   ha explotado vaso en mano que forma tan bonita empezar d√≠a   \n",
              "1033                                                                                  que kinox no quiere zi yo zoy buena perzona   \n",
              "1191                                                      isco con juande no seria titular y sabeis al mister le van mas tissones   \n",
              "799   he capturado mi primer gimnasio en pokemon go cierto es que es probable que no tarden ni 10 minutos en quitarmelo pero mola   \n",
              "1373                                mi padre le ha dado manotazo mi m√≥vil y gracias dios que que ha roto ha sido cristal templado   \n",
              "481                                           ojal√° justin subiendo foto comi√©ndose boca con sof√≠a para que os den por culo todas   \n",
              "645                                                               tiene que ser entretenido tu seras primera en verlo amore mio üòò   \n",
              "1261                                                               yo puedo cambiar opini√≥n cara es m√°s dif√≠cil porque vale pasta   \n",
              "941                                                                                                        y que sea yo es triste   \n",
              "\n",
              "     polaridad predicci√≥n  \n",
              "134          N          N  \n",
              "1246         N          P  \n",
              "1033         N          N  \n",
              "1191         N          N  \n",
              "799          P          N  \n",
              "1373         N          N  \n",
              "481          N          N  \n",
              "645          P          P  \n",
              "1261         N          N  \n",
              "941          N          N  "
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame({'texto':X_test, 'polaridad':y_test, 'predicci√≥n':prediccion}).sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otE1kSco5GYm"
      },
      "source": [
        "Precisi√≥n del modelo (# predicciones correctas / Total de muestras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJgOTzFl5GYm",
        "outputId": "b805b5e3-e6c0-4055-f888-f8c13fd5ea78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy (exactitud):  0.7215568862275449\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print('Accuracy (exactitud): ', accuracy_score(y_test, prediccion))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E71O0a0b5GYm"
      },
      "source": [
        "Matriz de confusi√≥n (predicci√≥n -columnas- frente a etiquetas reales -filas-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "Yn0ij_A15GYn",
        "outputId": "472e7879-bebb-4ab5-bedb-c5f57e58fde8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>N_pred</th>\n",
              "      <th>P_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>N_true</th>\n",
              "      <td>154</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>P_true</th>\n",
              "      <td>55</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        N_pred  P_pred\n",
              "N_true     154      38\n",
              "P_true      55      87"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test, prediccion)\n",
        "pd.DataFrame(cm, index=('N_true','P_true'), columns=('N_pred','P_pred'))\n",
        "#filas: True Label, columnas: Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtAWaT0u5GYn"
      },
      "source": [
        "### Veamos qu√© palabras son las m√°s relevantes en el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmEcrYaa5GYn",
        "outputId": "c45a2ee3-d374-42ca-93a6-8cb9cf4bd809"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Menores Coefs:\n",
            "['ni' 'no' 'puto' 'triste' 'alguien' 'puta' 'sad' 'porque' 'alg√∫n' 'eso']\n",
            "\n",
            "Mayores Coefs: \n",
            "['gran' 'gracias' 'genial' 'buena' 'bien' 'hacerlo' 'puedes' 'guapa'\n",
            " 'muchas' 'tranquila']\n"
          ]
        }
      ],
      "source": [
        "# obtenemos los nombres de las caracter√≠sticas numpy array\n",
        "feature_names = np.array(vect.get_feature_names_out())\n",
        "\n",
        "# Ordenamos los coeficientes del modelo\n",
        "sorted_coef_index = modelLR.coef_[0].argsort()\n",
        "\n",
        "# Listamos los 10 coeficientes menores y mayores\n",
        "print('Menores Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
        "print('Mayores Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdrU8yM35GYn"
      },
      "source": [
        "## Otros modelos\n",
        "Probamos con los modelos Na√Øve Bayes y un SGD lineal para ver si mejora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIZNi_-y5GYn"
      },
      "source": [
        "Los modelos MultinomialNB (Multinomial Naive Bayes) son una t√©cnica de aprendizaje autom√°tico que se utiliza principalmente en problemas de clasificaci√≥n de texto.\n",
        "El modelo SGDClassifier es un modelo de clasificaci√≥n lineal que utiliza el m√©todo de descenso de gradiente estoc√°stico como m√©todo de optimizaci√≥n para ajustar los pesos del modelo en el entrenamiento, por defecto aplica SVM (support vector machines)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "N2U2QUSM5GYn"
      },
      "outputs": [],
      "source": [
        "def train_predict_evaluate_model(classifier, \n",
        "                                 train_features, train_labels, \n",
        "                                 test_features, test_labels):\n",
        "    '''Funci√≥n que entrena y valida un clasificador sobre\n",
        "    un conjunto especificado de entrenamiento y test.\n",
        "    Devuelve la predicci√≥n sobre el conjunto de test'''\n",
        "    # entrena modelo    \n",
        "    classifier.fit(train_features, train_labels)\n",
        "    # predice en test usando el modelo\n",
        "    predictions = classifier.predict(test_features) \n",
        "    # eval√∫a el rendimiento del modelo   \n",
        "    print('Accuracy (exactitud): ', accuracy_score(test_labels, predictions))\n",
        "    return predictions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXgcSa0t5GYn",
        "outputId": "04e3cc6b-3947-4167-fc5b-26328fab8d54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo Multinomial Na√Øve Bayes:\n",
            "Accuracy (exactitud):  0.7514970059880239\n",
            "Modelo SVM lineal:\n",
            "Accuracy (exactitud):  0.6706586826347305\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.naive_bayes import MultinomialNB  # Multinomial naive bayes\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# creamos los modelos\n",
        "modelNB = MultinomialNB()\n",
        "modelSVM = SGDClassifier(loss='hinge', max_iter=10000, tol=1e-5) # loss/cost function, tol es el error a partir del cual deja de entrenar\n",
        "\n",
        "# entrenamos y evaluamos\n",
        "X_test_vectorized=vect.transform(X_test)\n",
        "print(\"Modelo Multinomial Na√Øve Bayes:\")\n",
        "prediccionNB = train_predict_evaluate_model(modelNB, \n",
        "                                 X_train_vectorized, y_train, \n",
        "                                 X_test_vectorized, y_test)\n",
        "print(\"Modelo SVM lineal:\")\n",
        "prediccionSVM = train_predict_evaluate_model(modelSVM, \n",
        "                                 X_train_vectorized, y_train, \n",
        "                                 X_test_vectorized, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCjrF2hg5GYo"
      },
      "source": [
        "### Ejercicio 1:\n",
        "Vuelve a entrenar el modelo Logistic Regression usando la funci√≥n `train_predict_evaluate_model` para comparar f√°cilmente con los resultados anteriores. ¬øcu√°l funciona mejor de los tres?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94MSti4R5GYo",
        "outputId": "c948675d-8074-46ee-c989-6ef944bc6961"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy (exactitud):  0.7215568862275449\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array(['N', 'N', 'N', 'P', 'N', 'N', 'P', 'N', 'P', 'N', 'N', 'P', 'P',\n",
              "       'P', 'N', 'N', 'N', 'P', 'P', 'P', 'N', 'P', 'P', 'P', 'N', 'N',\n",
              "       'N', 'P', 'P', 'N', 'P', 'P', 'P', 'P', 'N', 'N', 'N', 'N', 'N',\n",
              "       'P', 'P', 'N', 'N', 'N', 'N', 'N', 'P', 'N', 'N', 'N', 'N', 'P',\n",
              "       'P', 'P', 'P', 'P', 'N', 'P', 'N', 'N', 'P', 'N', 'P', 'P', 'N',\n",
              "       'N', 'P', 'N', 'P', 'N', 'P', 'P', 'P', 'N', 'N', 'N', 'P', 'P',\n",
              "       'N', 'N', 'N', 'P', 'P', 'P', 'N', 'N', 'N', 'P', 'N', 'N', 'N',\n",
              "       'N', 'N', 'N', 'N', 'P', 'N', 'P', 'N', 'P', 'N', 'N', 'P', 'N',\n",
              "       'N', 'N', 'P', 'N', 'P', 'N', 'P', 'P', 'N', 'N', 'N', 'P', 'P',\n",
              "       'N', 'N', 'N', 'N', 'N', 'P', 'N', 'P', 'N', 'N', 'N', 'P', 'N',\n",
              "       'P', 'N', 'P', 'N', 'N', 'P', 'N', 'P', 'N', 'P', 'N', 'N', 'N',\n",
              "       'N', 'P', 'N', 'P', 'P', 'P', 'P', 'P', 'N', 'P', 'N', 'N', 'P',\n",
              "       'P', 'P', 'P', 'N', 'N', 'N', 'P', 'N', 'N', 'P', 'N', 'N', 'N',\n",
              "       'N', 'P', 'P', 'N', 'P', 'N', 'P', 'N', 'N', 'P', 'P', 'P', 'P',\n",
              "       'P', 'N', 'N', 'P', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N',\n",
              "       'N', 'N', 'P', 'P', 'P', 'N', 'N', 'N', 'P', 'N', 'N', 'N', 'P',\n",
              "       'P', 'P', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'P', 'P', 'N', 'N',\n",
              "       'P', 'N', 'N', 'N', 'N', 'N', 'P', 'N', 'P', 'N', 'N', 'P', 'N',\n",
              "       'N', 'P', 'N', 'N', 'P', 'N', 'N', 'N', 'N', 'P', 'P', 'P', 'N',\n",
              "       'P', 'N', 'N', 'N', 'P', 'N', 'N', 'N', 'P', 'P', 'P', 'P', 'N',\n",
              "       'N', 'P', 'N', 'N', 'N', 'P', 'N', 'P', 'N', 'N', 'N', 'N', 'P',\n",
              "       'N', 'N', 'N', 'N', 'P', 'N', 'N', 'N', 'P', 'N', 'N', 'N', 'N',\n",
              "       'P', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'P', 'P', 'N',\n",
              "       'N', 'P', 'N', 'N', 'P', 'P', 'N', 'P', 'N', 'P', 'N', 'N', 'P',\n",
              "       'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N',\n",
              "       'N', 'N', 'P', 'P', 'P', 'N', 'N', 'N', 'P'], dtype=object)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### SOLUCI√ìN\n",
        "# Crea el modelo de regresi√≥n log√≠stica\n",
        "modelLR = LogisticRegression(solver='liblinear')\n",
        "\n",
        "# Entrena y eval√∫a el modelo utilizando la funci√≥n train_predict_evaluate_model\n",
        "train_predict_evaluate_model(modelLR, X_train_vectorized, y_train, X_test_vectorized, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVhWJYjEF7m4"
      },
      "source": [
        "El Multinomial Na√Øve Bayes es el que ha dado mejores resultados de los tres, pero a√∫n as√≠ son todos muy parecidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIGUl2q15GYo"
      },
      "source": [
        "## Modelo TF-IDF\n",
        "\n",
        "El modelo TF-IDF asigna un peso a cada palabra en un documento en funci√≥n de su frecuencia en el documento y en todo el corpus. La idea detr√°s del modelo es que las palabras que aparecen con frecuencia en un documento pero raramente en el corpus en general tienen un mayor peso y, por lo tanto, son m√°s importantes para ese documento en particular. Las palabras que aparecen con frecuencia en el corpus en general pero raramente en el documento, tienen un peso menor y, por lo tanto, se consideran menos importantes para ese documento en particular.\n",
        "\n",
        "EL modelo TF-IDF se implementa con la funci√≥n `TfidfVectorizer` de `scikit-learn`. Definimos una funci√≥n para calcular el modelo TF-IDF y extraer las caracter√≠sticas del conjunto de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "vQD8QpLF5GYo"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#definimos una funci√≥n para ajustar el modelo y extraer las caracter√≠sticas (palabras de la matriz)\n",
        "def tfidf_extractor(corpus):\n",
        "    '''Funci√≥n que genera un modelo TF-IDF sobre un corpus de texto\n",
        "    El corpus debe ser una lista de textos (palabras separadas por espacios)\n",
        "    Devuelve el modelo TF-IDF generado y el vector TF-IDF del corpus'''\n",
        "    \n",
        "    vectorizer = TfidfVectorizer()\n",
        "    features = vectorizer.fit_transform(corpus)\n",
        "    return vectorizer, features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6E0X2fi5GYo"
      },
      "source": [
        "Calculamos el modelo TF-IDF sobre el corpus de entrenamiento y con este modelo extraemos las matrices del conjunto de entrenamiento y de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "aR4HIM0D5GYo"
      },
      "outputs": [],
      "source": [
        "#Creamos los vectores de caracter√≠sticas TF-IDF\n",
        "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(X_train)  \n",
        "tfidf_test_features = tfidf_vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-amDQ7J5GYo",
        "outputId": "fce4745e-1f66-4a68-fc29-6d80fcae0aa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo Logistic Regression con caracter√≠sticas TF-IDF\n",
            "Accuracy (exactitud):  0.6946107784431138\n",
            "Modelo Naive Bayes con caracter√≠sticas TF-IDF\n",
            "Accuracy (exactitud):  0.6916167664670658\n",
            "Modelo Linear SVM con caracter√≠sticas TF-IDF\n",
            "Accuracy (exactitud):  0.718562874251497\n"
          ]
        }
      ],
      "source": [
        "#Entrenamos los 3 clasificadores con las caracter√≠sticas TF-IDF\n",
        "modelos = [('Logistic Regression', modelLR),\n",
        "           ('Naive Bayes', modelNB),\n",
        "           ('Linear SVM', modelSVM)]\n",
        "for m, clf in modelos:\n",
        "    print('Modelo {} con caracter√≠sticas TF-IDF'.format(m))\n",
        "    tfidf_predictions = train_predict_evaluate_model(classifier=clf,\n",
        "                                           train_features=tfidf_train_features,\n",
        "                                           train_labels=y_train,\n",
        "                                           test_features=tfidf_test_features,\n",
        "                                           test_labels=y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0T8XB8J5GYo"
      },
      "source": [
        "Obtenemos unos resultados algo peores a los que obtenemos con los modelos BoW.  \n",
        "Los mayores coeficientes para cada clase son:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp1dM7K_5GYo",
        "outputId": "3943e1ac-85bf-48c4-8378-7be64cc12c29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Menores Coefs:\n",
            "['no' 'porque' 'ni' 'estoy' 'eso' 'triste' 'he' 'sad' 'puta' 'puto']\n",
            "\n",
            "Mayores Coefs: \n",
            "['gracias' 'gran' 'feliz' 'd√≠a' 'buena' 'genial' 'bien' 'buen' 'mejor'\n",
            " 'guapa']\n"
          ]
        }
      ],
      "source": [
        "# obtenemos los nombres de las caracter√≠sticas numpy array\n",
        "feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Ordenamos los coeficientes del modelo\n",
        "sorted_coef_index = modelLR.coef_[0].argsort()  \n",
        "\n",
        "# Listamos los 10 coeficientes menores y mayores\n",
        "print('Menores Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
        "\n",
        "print('Mayores Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]])) # 10 √∫ltimos elementos, -1 significa en orden inverso\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlh5rean5GYp",
        "outputId": "f37c81dc-062b-4fa4-90d7-2ab16a660d32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(777, 3277)\n",
            "(334, 3277)\n"
          ]
        }
      ],
      "source": [
        "# Convierto tfidf_train_features a array para visualizar su contenido y Comprueba las dimensiones de las los conjuntos de caracter√≠sticas de entrenamiento y test\n",
        "train= tfidf_train_features.toarray()\n",
        "print(tfidf_train_features.shape)\n",
        "print(tfidf_test_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuManKVL5GYp"
      },
      "source": [
        "### Ejercicio 2:\n",
        "Crea una funci√≥n `bow_extractor` an√°loga a la funci√≥n `tfidf_extractor` para generar un modelo BoW a partir de un corpus de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "If2Uuzy_5GYp"
      },
      "outputs": [],
      "source": [
        "#definimos una funci√≥n para ajustar el modelo y extraer las caracter√≠sticas (palabras de la matriz)\n",
        "def bow_extractor(corpus):\n",
        "    '''Funci√≥n que genera un modelo BoW sobre un corpus de texto\n",
        "    El corpus debe ser una lista de textos (palabras separadas por espacios)\n",
        "    Devuelve el modelo BoW generado y el vector BoW del corpus'''\n",
        "    \n",
        "    vectorizer = CountVectorizer()\n",
        "    features = vectorizer.fit_transform(corpus)\n",
        "    return vectorizer, features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3D0oytw5GYp"
      },
      "source": [
        "## Modelos sobre texto lematizado\n",
        "Probamos a entrenar los clasificadores con el texto lematizado para ver si mejoramos los resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "sd_amIe65GYp"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['lemas'], \n",
        "                                                    df['polarity'],\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtOXGtVQ5GYp"
      },
      "source": [
        "### Ejercicio 3:\n",
        "Aplica los modelos de BoW y TF-IDF con las funciones definidas anteriormente para obtener las matrices de caracter√≠sticas del conjunto de entrenamiento y de test (texto lematizado). Tienes que calcular las matrices `bow_train_features` y `bow_test_features` con el modelo BoW y las matrices `tfidf_train_features` y `tfidf_test_features` sobre el modelo TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "lxz9ulXn5GYp"
      },
      "outputs": [],
      "source": [
        "## SOLUCI√ìN\n",
        "#Modelo BoW\n",
        "bow_vectorizer, bow_train_features = tfidf_extractor(X_train)  \n",
        "bow_test_features = bow_vectorizer.transform(X_test)\n",
        "\n",
        "#Modelo TF-IDF\n",
        "tfidf_vectorizer, tfidf_train_features = tfidf_extractor(X_train)  \n",
        "tfidf_test_features = tfidf_vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED6pOFcb5GYp",
        "outputId": "758b504d-7194-4125-91cc-a8e8dcd764c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2618\n",
            "2618\n"
          ]
        }
      ],
      "source": [
        "print(len(bow_vectorizer.get_feature_names_out()))\n",
        "print(len(tfidf_vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rUVtKnP5GYp"
      },
      "source": [
        "Observa que el n√∫mero de t√©rminos en el vocabulario se ha reducido notablemente al coger el lema de las palabras (muchos t√©rminos compart√≠an el mismo lema).  \n",
        "Probamos si se mejora con los 3 clasificadores que hemos usado anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97UBxeNg5GYp",
        "outputId": "fda8b995-0834-43d4-ce17-2d6f6d190748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo Logistic Regression con caracter√≠sticas BoW\n",
            "Accuracy (exactitud):  0.7604790419161677\n",
            "Modelo Naive Bayes con caracter√≠sticas BoW\n",
            "Accuracy (exactitud):  0.7365269461077845\n",
            "Modelo Linear SVM con caracter√≠sticas BoW\n",
            "Accuracy (exactitud):  0.7125748502994012\n"
          ]
        }
      ],
      "source": [
        "#entrenamos clasificadores con modelos BoW\n",
        "for m, clf in modelos:\n",
        "    print('Modelo {} con caracter√≠sticas BoW'.format(m))\n",
        "    bow_predictions = train_predict_evaluate_model(classifier=clf,\n",
        "                                           train_features=bow_train_features,\n",
        "                                           train_labels=y_train,\n",
        "                                           test_features=bow_test_features,\n",
        "                                           test_labels=y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-bM9e7S5GYq"
      },
      "source": [
        "Haz lo mismo para TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Esk9Wppi5GYq",
        "outputId": "1457c465-1750-4f17-9bbb-1a7d2cbf44c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo Logistic Regression con caracter√≠sticas TF-IDF\n",
            "Accuracy (exactitud):  0.7604790419161677\n",
            "Modelo Naive Bayes con caracter√≠sticas TF-IDF\n",
            "Accuracy (exactitud):  0.7365269461077845\n",
            "Modelo Linear SVM con caracter√≠sticas TF-IDF\n",
            "Accuracy (exactitud):  0.7305389221556886\n"
          ]
        }
      ],
      "source": [
        "# SOLUCI√ìN\n",
        "for m, clf in modelos:\n",
        "    print('Modelo {} con caracter√≠sticas TF-IDF'.format(m))\n",
        "    tfidf_predictions = train_predict_evaluate_model(classifier=clf,\n",
        "                                           train_features=tfidf_train_features,\n",
        "                                           train_labels=y_train,\n",
        "                                           test_features=tfidf_test_features,\n",
        "                                           test_labels=y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwlrDJRA5GYq"
      },
      "source": [
        "Vemos que la clasificaci√≥n s√≠ que ha mejorado.  \n",
        "Vemos cu√°les son las caracter√≠sticas m√°s importantes para el modelo LR sobre TF-IDF:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msPDaHPW5GYq",
        "outputId": "81917264-7e16-4428-cc3c-617edef4d2b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Menores Coefs:\n",
            "['no' 'porque' 'ni' 'triste' 'ese' 'poner' 'malo' 'sad' 'pobre' 'puto']\n",
            "\n",
            "Mayores Coefs: \n",
            "['buen' 'gran' 'gracia' 'genial' 'feliz' 'tranquilo' 'mejor' 'mucho'\n",
            " 'bien' 'guapo']\n"
          ]
        }
      ],
      "source": [
        "# obtenemos los nombres de las caracter√≠sticas numpy array\n",
        "feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Ordenamos los coeficientes del modelo\n",
        "sorted_coef_index = modelLR.coef_[0].argsort()\n",
        "\n",
        "# Listamos los 10 coeficientes menores y mayores\n",
        "print('Menores Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
        "print('Mayores Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfuKzHap5GYq"
      },
      "source": [
        "### EJERCICIO 4\n",
        "Haz un an√°lisis de subjetividad de textos, para ello trata de capturar texto de diferentes p√°ginas web y compara el nivel de subjetividad de cada uno de ellos. Investiga qu√© librer√≠as existen en python para realizar dicho an√°lisis. ¬øQu√© √≠ndices proporcionan estas librer√≠as y en qu√© se basan?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaiyT_qTP1rh"
      },
      "source": [
        "--------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgViN4r_PUmo"
      },
      "source": [
        "Existen diversas librer√≠as en Python que permiten realizar an√°lisis de subjetividad de textos. Algunas de las m√°s populares son:\n",
        "\n",
        "* NLTK (Natural Language Toolkit): una librer√≠a de procesamiento de lenguaje natural que incluye herramientas para realizar an√°lisis de sentimiento y subjetividad. Ofrece modelos pre-entrenados para distintos idiomas y dominios, as√≠ como funciones para entrenar modelos propios.\n",
        "\n",
        "* TextBlob: una librer√≠a que ofrece una interfaz sencilla para realizar an√°lisis de sentimiento y subjetividad. Incluye modelos pre-entrenados para ingl√©s y permite entrenar modelos propios a partir de corpus etiquetados.\n",
        "\n",
        "* VaderSentiment: una librer√≠a especializada en an√°lisis de sentimiento para redes sociales y otros textos informales. Utiliza un conjunto de reglas heur√≠sticas para identificar expresiones positivas, negativas y neutrales.\n",
        "\n",
        "* Pattern: una librer√≠a que ofrece diversas funcionalidades para procesamiento de lenguaje natural, incluyendo an√°lisis de sentimiento y subjetividad. Utiliza un modelo probabil√≠stico basado en el an√°lisis de frecuencia de palabras y expresiones.\n",
        "\n",
        "Cada una de estas librer√≠as proporciona distintos √≠ndices y medidas para el an√°lisis de subjetividad. Algunos de los m√°s comunes son:\n",
        "\n",
        "* Polaridad: indica el grado de positividad o negatividad de un texto, en una escala que va de -1 a 1.\n",
        "\n",
        "* Subjetividad: indica el grado de subjetividad o objetividad de un texto, en una escala que va de 0 a 1.\n",
        "\n",
        "* Intensidad: indica el grado de intensidad emocional de un texto, en una escala que va de 0 a 1.\n",
        "\n",
        "Estos √≠ndices se basan en modelos estad√≠sticos y algoritmos de aprendizaje autom√°tico que han sido entrenados con _corpus_ etiquetados. Cada librer√≠a utiliza distintas estrategias y enfoques para realizar el an√°lisis de subjetividad, por lo que es importante evaluar su desempe√±o en distintos tipos de textos y contextos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-v5tGyaPTsX",
        "outputId": "4e73a42f-fe38-4e88-f595-289ce407771f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to\n",
            "[nltk_data]    |     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCEgWD_gQ3Oi"
      },
      "source": [
        "Vamos a analizar una misma noticia presentada en diferentes peri√≥dicos. Para ello, hemos de tener en cuenta los siguientes listados:\n",
        "\n",
        "_Peri√≥dicos de tendencia pol√≠tica de izquierda_:\n",
        "\n",
        "* El Pa√≠s\n",
        "* P√∫blico\n",
        "* La Vanguardia (centro-izquierda)\n",
        "* El Diario\n",
        "* Infolibre\n",
        "* El Salto\n",
        "\n",
        "_Peri√≥dicos de tendencia pol√≠tica de derecha_:\n",
        "\n",
        "* ABC\n",
        "* El Mundo\n",
        "* La Raz√≥n\n",
        "* Libertad Digital\n",
        "* Okdiario\n",
        "* El Espa√±ol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kj83hmJzScsh"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "K6p6dE7EPpr-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'neg': 0.0, 'neu': 0.99, 'pos': 0.01, 'compound': 0.5859}\n"
          ]
        }
      ],
      "source": [
        "# Noticia de El Pa√≠s\n",
        "url_1 = \"https://elpais.com/sociedad/2023-04-05/la-fiscalia-archiva-la-investigacion-por-los-canticos-machistas-del-elias-ahuja.html \"\n",
        "\n",
        "# Noticia Libertad_digital\n",
        "url_2 =\"https://www.libertaddigital.com/madrid/2023-04-05/la-fiscalia-archiva-la-investigacion-por-los-canticos-del-colegio-mayor-elias-ahuja-al-no-constituir-delito-de-odio-7002206/\"\n",
        "\n",
        "try:\n",
        "    page = requests.get(url_1)\n",
        "except:\n",
        "    print(\"Error al abrir la URL\")\n",
        "\n",
        "soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "# Buscamos el <div> correspondiente y sacamos su contenido:\n",
        "content_1 = soup.find('div', {\"class\": \"a_c clearfix\"})\n",
        "article_1 = []\n",
        "for i in content_1.find_all('p'):\n",
        "    article_1.append(i.text)\n",
        "\n",
        "article_1 = ''.join(article_1)\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')  # tokenizador para separar en palabras\n",
        "stop_words = set(stopwords.words('spanish'))  # lista de stopwords\n",
        "lemmatizer = WordNetLemmatizer()  # lematizador\n",
        "\n",
        "# Separar el texto en palabras\n",
        "words = tokenizer.tokenize(article_1.lower())\n",
        "\n",
        "# Filtrar stopwords y lematizar las palabras\n",
        "words = [lemmatizer.lemmatize(word, pos='v') for word in words if word not in stop_words]\n",
        "\n",
        "# Analizar la subjetividad del texto\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "polarity_scores = sia.polarity_scores(' '.join(words))\n",
        "\n",
        "print(polarity_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'neg': 0.0, 'neu': 0.992, 'pos': 0.008, 'compound': 0.2732}\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    page = requests.get(url_2)\n",
        "except:\n",
        "    print(\"Error al abrir la URL\")\n",
        "\n",
        "soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "# Buscamos el <div> correspondiente y sacamos su contenido:\n",
        "content_2 = soup.find('div', {\"class\": \"body\"})\n",
        "article_2 = []\n",
        "for i in content_2.find_all('p'):\n",
        "    article_2.append(i.text)\n",
        "\n",
        "article_2 = ''.join(article_2)\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')  # tokenizador para separar en palabras\n",
        "stop_words = set(stopwords.words('spanish'))  # lista de stopwords\n",
        "lemmatizer = WordNetLemmatizer()  # lematizador\n",
        "\n",
        "# Separar el texto en palabras\n",
        "words = tokenizer.tokenize(article_2.lower())\n",
        "\n",
        "# Filtrar stopwords y lematizar las palabras\n",
        "words = [lemmatizer.lemmatize(word, pos='v') for word in words if word not in stop_words]\n",
        "\n",
        "# Analizar la subjetividad del texto\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "polarity_scores = sia.polarity_scores(' '.join(words))\n",
        "\n",
        "print(polarity_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La Fiscal√≠a archiva la investigaci√≥n por los c√°nticos machistas del colegio mayor El√≠as Ahuja | Educaci√≥n | EL PA√çSSeleccione:- - -Espa√±aAm√©ricaM√©xicoColombiaChileArgentinaUSAEducaci√≥nsuscr√≠beteHHOLAIniciar sesi√≥nEducaci√≥nInfantil y PrimariaSecundaria, Bachillerato y FPUniversidades√öltimas noticiasMACHISMOLa Fiscal√≠a archiva la investigaci√≥n por los c√°nticos machistas del colegio mayor El√≠as AhujaEl ministerio p√∫blico considera que las expresiones fueron ‚Äúirrespetuosas e insultantes para las mujeres‚Äù, pero no constituyen un delito de odio00:52\"Putas, salid de vuestras madrigueras como conejas\"Entrada al colegio mayor masculino El√≠as Ahuja, adscrito a la Universidad Complutense de Madrid.\n",
            "Foto: CLAUDIO √ÅLVAREZ | V√≠deo: EPVElisa Sili√≥Luc√≠a Boh√≥rquezMadrid / Palma - 05 abr 2023 - 09:37Actualizado: 05 abr 2023 - 10:02 UTCWhatsappFacebookTwitterCopiar enlaceComentariosLa Fiscal√≠a de Madrid ha acordado archivar las diligencias de investigaci√≥n abiertas contra un alumno de la residencia de estudiantes El√≠as Ahuja de Madrid por los gritos sexistas lanzados la noche del 2 de octubre de 2022 a las residentes del colegio mayor femenino contiguo Santa M√≥nica, seg√∫n ha informado este mi√©rcoles el ministerio p√∫blico. Las diligencias se abrieron a ra√≠z de una denuncia del Movimiento contra la Intolerancia al considerar que los hechos podr√≠an ser constitutivos de un delito de odio.Se da la circunstancia de que la ley del solo s√≠ es s√≠, que regula este tipo de conductas, entr√≥ en vigor el siguiente viernes, 7 de octubre. Esta norma castiga a quienes se dirijan a otra persona con expresiones, comportamientos o proposiciones de car√°cter sexual que creen en la v√≠ctima una situaci√≥n objetivamente humillante, hostil o intimidatoria, sin llegar a constituir otros delitos de mayor gravedad.En febrero, el alumno que inici√≥ los c√°nticos que pronto se viralizaron ‚Äï‚Äú¬°Putas, salid de vuestras madrigueras como conejas, sois unas putas ninf√≥manas, os prometo que vais a follar todas en la capea! ¬°Vamos, Ahuja!‚Äù‚Äï, a los que se sumaron el resto de estudiantes simulando ser animales con sus ruidos, manifest√≥ ante el fiscal que los gritos hacia sus vecinas del Santa M√≥nica ‚Äïmuchas de ellas familiares‚Äï eran ‚Äúuna broma‚Äù que segu√≠a ‚Äúuna tradici√≥n‚Äù, negando que su intenci√≥n fuera humillar a las chicas. La escena, que se conoce como La Granja, se repet√≠a cada a√±o y las colegialas no se sintieron agredidas. ‚ÄúA m√≠ si me llaman puta o ninf√≥mana por la calle, claro que me ofendo, pero ellos son nuestros amigos‚Äù, subray√≥ una joven a este diario.El fiscal jefe no apreci√≥ tras la investigaci√≥n ‚Äúvinculaci√≥n alguna con grupos o movimientos extremistas‚Äù previa o posterior del alumno que inici√≥ unos c√°nticos que tilda de ‚Äúsoeces y procaces‚Äù.  Y afirma en sus diligencias que el colegio mayor no hab√≠a podido ‚Äúidentificar a los estudiantes que estaban en las instalaciones en ese momento debido a que el sistema de autorizaciones para salidas nocturnas es habilitado para todo el curso escolar‚Äù. Tampoco a quienes grabaron las im√°genes que pronto se viralizaron.El decreto de archivo del fiscal sostiene que los hechos son ‚Äúirrespetuosos e insultantes para las mujeres‚Äù y las expresiones proferidas constituyen ‚Äúun ataque a la dignidad individual o colectiva de aquellas‚Äù. Sin embargo, no pueden ser por s√≠ solas constitutivas de un delito de odio del art√≠culo 510.2 del C√≥digo Penal, al exigir este delito la concurrencia de una motivaci√≥n discriminatoria concreta, la cual no ha resultado acreditada en la investigaci√≥n por hechos anteriores, coet√°neos ni posteriores a los denunciados. Seg√∫n la Fiscal√≠a, la acci√≥n investigada no puede tipificarse tampoco como un delito contra la integridad moral porque para ello es necesario que alguna de las personas destinatarias de las expresiones proferidas se hubiera sentido ofendida y ‚Äúno consta que ninguna de las mujeres que se encontraban en la residencia haya denunciado los hechos‚Äù.Aunque la conducta de los colegiales fue condenada por todo el arco pol√≠tico, la presidenta de la Comunidad de Madrid, Isabel D√≠az Ayuso, se desmarc√≥ entonces criticando a la Fiscal√≠a: ‚ÄúA m√≠ lo que me sorprende, sobre todo, es que la Fiscal√≠a est√© para investigar esto, mientras en la Universidad, a lo largo de los a√±os, hemos visto en numerosas ocasiones pancartas a favor de los presos de ETA, hemos visto c√≥mo han acosado y han montado escraches a profesores y alumnos impidiendo conferencias en libertad, o persiguen, por ejemplo, a los alumnos de S‚Äôha acabat para que no puedan ir libremente a su universidad, a la facultad en Catalu√±a‚Äù.Cambios en la ley universitariaEl incidente machista del Ahuja no se ha reducido a objeto de debate en los medios durante d√≠as. A iniciativa de M√°s Pa√≠s, la Ley Org√°nica del Sistema Universitario incluye un art√≠culo que obliga a los colegios mayores ‚Äïsupuestamente no son solo un alojamiento, sino que forman a los universitarios‚Äï a ser mixtos si no quieren ser expulsados de la red p√∫blica. Si siguen segregando, pasar√°n a ser residencias privadas, lo que les resta prestigio. Los colegios de titularidad p√∫blica hace tiempo que no separan, y en los privados como el Ahuja el proceso est√° siendo m√°s largo.En los debates de las recientes elecciones a rector de la Complutense, como no pod√≠a ser de otra manera ‚Äïha sido uno de sus √∫ltimos golpes reputacionales‚Äï, sali√≥ a colaci√≥n el canon que los agustinos pagan a la universidad como contraprestaci√≥n por tener el colegio en sus terrenos: 60.000 euros al a√±o. Una cantidad peque√±a si se tiene en cuenta que cada uno de los 174 colegiales paga 1.200 euros mensuales durante nueve meses (1,87 millones). Y esta cuenta no incluye las estancias en verano de otros hu√©spedes. ‚ÄúEn los colegios mayores que antes no se pagaba, ahora se paga‚Äù, razon√≥ el rector Joaqu√≠n Goyache en conversaci√≥n con este diario. ‚ÄúSi vas al portal de transparencia, ves que hay residencias universitarias que pagan cerca de 500.000 euros y otros colegios mayores que rondan los 200.000 euros‚Äù, a√±adi√≥ la entonces candidata a rectora Esther del Campo.La justicia ha archivado el caso y el castigo tambi√©n ha sido muy leve en el colegio para el estudiante de la Aut√≥noma de Madrid que inici√≥ los c√°nticos. Aunque la direcci√≥n del centro hab√≠a anunciado a los medios que su expulsi√≥n ser√≠a definitiva, luego se ech√≥ atr√°s. ‚ÄúEl reglamento del centro por estos hechos prev√© una expulsi√≥n de 15 d√≠as. El colegio lo ampli√≥ hasta 40 d√≠as y retir√≥ al colegial una beca honor√≠fica que ten√≠a (...) Se ha puesto la sanci√≥n m√°s alta que se ha podido seg√∫n el reglamento‚Äù, asegur√≥ a este diario. Sin embargo, el colegial, abrumado por su notoriedad, opt√≥ por alternar su estancia entre la residencia y casa de amigos en la capital.Puedes seguir EL PA√çS EDUCACI√ìN en Facebook y Twitter, o apuntarte aqu√≠ para recibir nuestra newsletter semanal.ComentariosNormas M√°s informaci√≥nEl dif√≠cil camino para que no se repitan los gritos del El√≠as Ahuja: ‚ÄúSois unas pedazo de mierdas. ¬°Putas!‚ÄùManuel Viejo / Elisa Sili√≥ | MadridLa nueva ley universitaria castigar√° a los colegios mayores que sigan segregando por sexos, como el El√≠as AhujaElisa Sili√≥ | MadridArchivado EnEducaci√≥nMachismoSociedadMujeresHombresUniversidadSexismoFiscal√≠aComunidad de MadridMadridEspa√±aDelitos odioInjuriasResidencias de EstudiantesUCMColegios mayoresEstudiantesSe adhiere a los criterios deM√°s informaci√≥nSi est√° interesado en licenciar este contenido contacte con ventacontenidos@prisamedia.comnewsletterRecibe el bolet√≠n de educaci√≥nESPECIAL PUBLICIDADLa nueva Formaci√≥n Profesional, una pasarela al empleoLo m√°s vistoTrabajar en el ‚Äòbig data‚Äô: ‚ÄúNunca he tenido que echar curr√≠culum, las ofertas me han llegado solas‚ÄùEl esc√°ndalo de los suspensos en el examen para dentistas extranjeros: ‚ÄúEs una burla, una estafa‚ÄùR√©cord de ‚Äòsisis‚Äô, m√°s personas que nunca estudian y trabajan a la vez: ‚ÄúEs duro. Al final te pasa factura‚ÄùAprender animaci√≥n y efectos especiales en Canarias con ‚ÄòStar Wars‚Äô y ‚ÄòLa casa del drag√≥n‚ÄôEl Constitucional avala que se nieguen las subvenciones a los centros que segreguen por sexo Recomendaciones EL PA√çSDescuentosCursosCursos onlineEntradasIngl√©s onlineEscaparateCrucigramas & Juegos ColeccionesDescuentosdescuentosUtiliza nuestro cup√≥n AliExpress y ah√≥rrate hasta un 50%descuentosAprovecha el c√≥digo promocional El Corte Ingl√©s y paga hasta un 50% menosdescuentosDisfruta del c√≥digo promocional Amazon y consigue hasta 20% de descuentodescuentosCanjea el c√≥digo descuento Groupon y paga un 20% menosCursoscursos¬øTe gustar√≠a especializarte en Adiestramiento y Est√©tica animal? ¬°Te ayudamos a encontrar los mejores cursos y formaci√≥n profesional!cursosMBA 'online' con un 86% de descuento y acceso a bolsa de empleo. ¬°Solicita m√°s informaci√≥n!cursosEncuentra aqu√≠ los mejores cursos y formaci√≥n profesional para especializarte en Diet√©tica y Nutrici√≥ncursos¬øTe gustar√≠a especializarte en Cocina, Reposter√≠a y Enolog√≠a? ¬°Te ayudamos a encontrar los mejores cursos y formaci√≥n profesional!Cursos onlinecursosonline¬øQuieres especializarte en Odontolog√≠a e Higiene Bucodental?  ¬°Encuentra los mejores cursos y formaci√≥n profesional aqu√≠!cursosonlineEncuentra aqu√≠ los mejores cursos FP 'online' y a distancia en Instalaciones El√©ctricas, T√©rmicas y de Gas cursosonline¬øQuieres especializarte en Mec√°nica? ¬°Encuentra los mejores cursos y formaci√≥n profesional 'online' y a distancia aqu√≠!cursosonlineM√°ster a distancia en Psicolog√≠a Infantil y Juvenil con un 86% de descuento y acceso a bolsa de empleoEntradasentradas'WE WILL ROCK YOU'. Disfruta con los grandes √©xitos de Queen. Hasta el 28 de mayo en Madridentradas'EL FANTASMA DE LA √ìPERA'. Llega a Madrid el musical m√°s exitoso de la historiaentradasFestival 'R√çO BABEL 2023'. Con Morat, Julieta Venegas... Del 30 de junio al 2 de julio, Madridentradas'FITO P√ÅEZ' en Ic√≥nica Sevilla Fest 2023. Jueves 6 de julio. Entradas ya a la venta Ingl√©s onlinecursosinglesMejora tu ingl√©s con EL PA√çS con 15 minutos al d√≠acursosinglesDisfruta de nuestras lecciones personalizadas, breves y divertidascursosinglesEval√∫a tu nivel y obt√©n un certificadocursosinglesPrueba 21 d√≠as gratis y sin compromisoEscaparateescaparateUn malet√≠n de llaves y destornillador superventas por menos de 30‚Ç¨escaparate12 cremas milagrosas para curar cicatricesescaparateProbamos los mejores amplificadores wifi de enchufeescaparateLa luz trasera para bici que arrasa en AmazonCrucigramas & Juegos juegosCrucigramas para expertosjuegosCrucigramas minisjuegosCrucigramas de Tarkus. juegosSopas de letras tem√°ticas JurjoColeccionescoleccionesLos mejores libros de la literatura universal recopilados en una colecci√≥n √∫nicacolecciones'El hombre invisible', 'Otra vuelta de tuerca' y 'El coraz√≥n de las tinieblas' entre otroscoleccionesAprovecha esta oportunidadcoleccionesY hazte con la colecci√≥n completaRecomendaciones EL PA√çSCursosCentrosFranc√©s onlineIngl√©s onlineItaliano onlineAlem√°n onlineCrucigramas & Juegos CursoscursosMaestr√≠a en Big Data y Analytics 100% en l√≠neacursosMBA Administraci√≥n y Direcci√≥n de Empresas en l√≠neacursosPrograma en l√≠nea en 'Project Finance' InternacionalcursosMaestr√≠a en l√≠nea en Direcci√≥n de Recursos Humanos y Gesti√≥n del TalentoCentroscursosonlineMaestr√≠a en l√≠nea en Gesti√≥n AmbientalcursosonlineMaestr√≠a en Comercio Internacional presencial en Madrid, Espa√±acursosonlineMaestr√≠a en Marketing Digital & E-Commerce. Semipresencial en Madrid, Espa√±acursosonlineMaestr√≠a en 'Data Management' e Innovaci√≥n Tecnol√≥gica 100% en l√≠neaFranc√©s onlinecursosinglesMejora tu franc√©s con 15 minutos al d√≠acursosinglesDisfruta de nuestras lecciones personalizadas, breves y divertidas.cursosinglesObtendr√° un diploma con estad√≠sticas de nivel, progresi√≥n y participaci√≥n. cursosingles 21 d√≠as de prueba gratuita de nuestro curso de franc√©s ‚Äòonline‚ÄôIngl√©s onlinecursosinglesMejore su ingl√©s con EL PA√çS con 15 minutos al d√≠acursosinglesDisfrute de nuestras lecciones personalizadas, breves y divertidascursosinglesEval√∫e su nivel y obtenga un certificadocursosinglesPruebe 21 d√≠as gratis y sin compromisoItaliano onlinecursosinglesMejore su italiano con EL PA√çS con 15 minutos al d√≠acursosinglesDisfrute de nuestras lecciones personalizadas, breves y divertidascursosinglesEval√∫e su nivel y obtenga un certificadocursosinglesPruebe 21 d√≠as gratis y sin compromisoAlem√°n onlinecursosinglesLas mejores oportunidades hablan alem√°n. Nuevo curso 'online' cursosinglesDisfrute de nuestras lecciones personalizadas, breves y divertidascursosinglesEval√∫e su nivel y obtenga un certificadocursosinglesPruebe 21 d√≠as gratis y sin compromisoCrucigramas & Juegos juegosCrucigramas minisjuegosCrucigramas TarkusjuegosSudokus minijuegosSopas de letras  Recomendaciones EL PA√çSCursosCentros Italiano onlineFranc√©s onlineAlem√°n onlineCrucigramas & Juegos CursoscursosMaestr√≠a en Ciencias Ambientales presencial en Benito Ju√°rezcursosLicenciatura en Administraci√≥n de Empresas presencial en Benito Ju√°rezcursosMaestr√≠a a distancia en Ling√º√≠stica Aplicada a la Ense√±anza del Espa√±ol como Lengua ExtranjeracursosLicenciatura Ejecutiva en Psicolog√≠a Semipresencial. Cinco sedes disponiblesCentros cursosonlineLicenciatura Ejecutiva en Derecho. Semipresencial en AguascalientescursosonlineMaestr√≠a a distancia en Actividad F√≠sica y SaludcursosonlineMaestr√≠a a distancia en Energ√≠as RenovablescursosonlineDescubre un completo Directorio de Centros de Formaci√≥nItaliano onlinecursosinglesMejore su italiano con solo 15 minutos al d√≠acursosinglesDisfrute de nuestras lecciones personalizadas, breves y divertidas.cursosinglesObtendr√° un diploma con estad√≠sticas de nivel, progresi√≥n y participaci√≥n.cursosingles21 d√≠as de prueba gratuita. ¬°Empiece ya!Franc√©s onlinecursosinglesMejore su franc√©s con solo 15 minutos al d√≠acursosinglesDisfrute de nuestras lecciones personalizadas, breves y divertidas.cursosinglesObtendr√° un diploma con estad√≠sticas de nivel, progresi√≥n y participaci√≥n.cursosingles21 d√≠as de prueba gratuita. ¬°Empiece ya!Alem√°n onlinecursosinglesLas mejores oportunidades hablan alem√°n. Nuevo curso 'online' cursosinglesDisfrute de nuestras lecciones personalizadas, breves y divertidas.cursosinglesObtendr√° un diploma con estad√≠sticas de nivel, progresi√≥n y participaci√≥n.cursosingles21 d√≠as de prueba gratuita. ¬°Empiece ya!Crucigramas & Juegos juegos¬°Disfruta con nuestros Crucigramas para expertos!juegosResuelve los √∫ltimos Crucigramas de MambrinojuegosJuega a nuestros Sudoku para Expertos y mejora d√≠a a d√≠a tu niveljuegosJuega a las nuevas Sopas de letras cl√°sicas y tem√°ticas de EL PA√çSReg√≠strate gratis para seguir leyendoINICIA SESI√ìNCREAR CUENTAO suscr√≠bete para leer sin l√≠mitesInicia sesi√≥n o reg√≠strate gratis para continuar leyendo en inc√≥gnitoReg√≠strate gratisInicia sesi√≥nSuscr√≠bete y lee sin l√≠mitesVer opciones de suscripci√≥nHOLAsuscr√≠bete por 1‚Ç¨Mi actividadMi suscripci√≥nMis datosMis newslettersDerechos y bajaExperiencias para m√≠desconectaralto contraste:BuscarSeleccione:- - -Espa√±aAm√©ricaM√©xicoColombiaChileArgentinaUSASi quieres seguir toda la actualidad sin l√≠mites, √∫nete a EL PA√çS por 1‚Ç¨ el primer mesSUSCR√çBETE AHORAInternacionalOpini√≥nEditorialesTribunasVi√±etasCartas a la DirectoraDefensora del lectorEspa√±aAndaluc√≠aCatalu√±aComunidad ValencianaGaliciaMadridPa√≠s VascoEconom√≠aMercadosViviendaMis derechosFormaci√≥nSociedadEducaci√≥nClima y Medio AmbienteCienciaSaludTecnolog√≠aCulturaDeportesTelevisi√≥nGente y Estilo de vidaFotograf√≠aV√≠deosPodcastsel pa√≠s semanalideasNegociosBabeliaQuadernel viajeroplaneta futuros modaiconGastroel comidistacinco d√≠asMotorMamas & PapasAm√©rica FuturaEscaparateCrucigramas y JuegosNewsletter√öltimas noticiasHemerotecaEspecialesServiciosDescuentosColeccionesS√≠guenos en: Comentarios NormasSuscr√≠bete en El Pa√≠s para participarYa tengo una suscripci√≥n\n",
            "{'neg': 0.026, 'neu': 0.961, 'pos': 0.014, 'compound': -0.7269}\n",
            "['fiscal', 'madr', 'acord', 'archiv', 'diligent', 'investig', 'abiert', 'alumn', 'resident', 'estudi', 'eli', 'ahuj', 'madr', 'grit', 'sexist', 'lanz', 'noch', '2', 'octubr', '2022', 'resident', 'colegi', 'mayor', 'femenin', 'contigu', 'sant', 'monic', 'segun', 'inform', 'miercol', 'ministeri', 'public', 'diligent', 'abrieron', 'raiz', 'denunci', 'movimient', 'intoler', 'consider', 'hech', 'podr', 'ser', 'constitut', 'delit', 'odi', 'da', 'circunst', 'ley', 'sol', 'regul', 'tip', 'conduct', 'entro', 'vigor', 'siguient', 'viern', '7', 'octubr', 'norm', 'castig', 'dirij', 'person', 'expresion', 'comport', 'proposicion', 'caract', 'sexual', 'cre', 'victim', 'situacion', 'objet', 'humill', 'hostil', 'intimidatori', 'lleg', 'constitu', 'delit', 'mayor', 'graved', 'febrer', 'alumn', 'inic', 'cantic', 'pront', 'viraliz', 'put', 'sal', 'madriguer', 'conej', 'unas', 'put', 'ninfoman', 'promet', 'vais', 'foll', 'tod', 'cape', 'vam', 'ahuj', 'sum', 'rest', 'estudi', 'simul', 'ser', 'animal', 'ruid', 'manifest', 'fiscal', 'grit', 'haci', 'vecin', 'sant', 'monic', 'much', 'familiar', 'brom', 'segu', 'tradicion', 'neg', 'intencion', 'humill', 'chic', 'escen', 'conoc', 'granj', 'repet', 'cad', 'a√±o', 'colegial', 'sint', 'agred', 'si', 'llam', 'put', 'ninfoman', 'call', 'clar', 'ofend', 'amig', 'subray', 'jov', 'diari', 'fiscal', 'jef', 'aprec', 'tras', 'investig', 'vincul', 'algun', 'grup', 'movimient', 'extrem', 'previ', 'posterior', 'alumn', 'inic', 'cantic', 'tild', 'soec', 'procac', 'afirm', 'diligent', 'colegi', 'mayor', 'pod', 'identific', 'estudi', 'instal', 'moment', 'deb', 'sistem', 'autoriz', 'sal', 'nocturn', 'habilit', 'curs', 'escol', 'tampoc', 'grab', 'imagen', 'pront', 'viraliz', 'decret', 'archiv', 'fiscal', 'sostien', 'hech', 'irrespetu', 'insult', 'mujer', 'expresion', 'profer', 'constitu', 'ataqu', 'dignid', 'individual', 'colect', 'aquell', 'embarg', 'pued', 'ser', 'sol', 'constitut', 'delit', 'odi', 'articul', '510', '2', 'codig', 'penal', 'exig', 'delit', 'concurrent', 'motiv', 'discriminatori', 'concret', 'result', 'acredit', 'investig', 'hech', 'anterior', 'coetane', 'posterior', 'denunci', 'segun', 'fiscal', 'accion', 'investig', 'pued', 'tipific', 'tampoc', 'delit', 'integr', 'moral', 'ello', 'necesari', 'algun', 'person', 'destinatari', 'expresion', 'profer', 'ofend', 'const', 'ningun', 'mujer', 'encontr', 'resident', 'denunci', 'hech', 'aunqu', 'conduct', 'colegial', 'conden', 'arco', 'polit', 'president', 'comun', 'madr', 'isabel', 'diaz', 'ayus', 'desmarc', 'entonc', 'critic', 'fiscal', 'sorprend', 'fiscal', 'investig', 'mientr', 'univers', 'larg', 'a√±os', 'vist', 'numer', 'ocasion', 'pancart', 'favor', 'pres', 'eta', 'vist', 'com', 'acos', 'mont', 'escrach', 'profesor', 'alumn', 'impid', 'conferent', 'libert', 'persig', 'ejempl', 'alumn', 's', 'acabat', 'pued', 'ir', 'librement', 'univers', 'facult', 'catalu√±', 'incident', 'machist', 'ahuj', 'reduc', 'objet', 'debat', 'medi', 'dias', 'inici', 'pais', 'ley', 'organ', 'sistem', 'universitari', 'inclu', 'articul', 'oblig', 'colegi', 'mayor', 'supuest', 'sol', 'aloj', 'sin', 'form', 'universitari', 'ser', 'mixt', 'si', 'quier', 'ser', 'expuls', 'red', 'public', 'si', 'sig', 'segreg', 'pas', 'ser', 'resident', 'priv', 'rest', 'prestigi', 'colegi', 'titular', 'public', 'hac', 'tiemp', 'sep', 'priv', 'ahuj', 'proces', 'siend', 'larg', 'debat', 'recient', 'eleccion', 'rector', 'complutens', 'pod', 'ser', 'maner', 'sid', 'ultim', 'golp', 'reputacional', 'sal', 'colacion', 'canon', 'agustin', 'pag', 'univers', 'contraprest', 'ten', 'colegi', 'terren', '60', '000', 'eur', 'a√±o', 'cantid', 'peque√±', 'si', 'cuent', 'cad', '174', 'colegial', 'pag', '1', '200', 'eur', 'mensual', 'nuev', 'mes', '1', '87', 'millon', 'cuent', 'inclu', 'estanci', 'veran', 'huesped', 'colegi', 'mayor', 'pag', 'ahor', 'pag', 'razon', 'rector', 'joaquin', 'goyach', 'convers', 'diari', 'si', 'vas', 'portal', 'transparent', 'ves', 'resident', 'universitari', 'pag', 'cerc', '500', '000', 'eur', 'colegi', 'mayor', 'rond', '200', '000', 'eur', 'a√±ad', 'entonc', 'candidat', 'rector', 'esther', 'camp', 'justici', 'archiv', 'cas', 'castig', 'sid', 'lev', 'colegi', 'estudi', 'autonom', 'madr', 'inic', 'cantic', 'aunqu', 'direccion', 'centr', 'anunci', 'medi', 'expulsion', 'definit', 'lueg', 'echo', 'atras', 'reglament', 'centr', 'hech', 'prev', 'expulsion', '15', 'dias', 'colegi', 'ampli', '40', 'dias', 'retir', 'colegial', 'bec', 'honorif', 'puest', 'sancion', 'alta', 'pod', 'segun', 'reglament', 'asegur', 'diari', 'embarg', 'colegial', 'abrum', 'notoried', 'opto', 'altern', 'estanci', 'resident', 'cas', 'amig', 'capital', 'pued', 'segu', 'pais', 'educ', 'facebook', 'twitt', 'apuntart', 'aqu', 'recib', 'newslett', 'semanal']\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "url = \"https://elpais.com/sociedad/2023-04-05/la-fiscalia-archiva-la-investigacion-por-los-canticos-machistas-del-elias-ahuja.html\"\n",
        "\n",
        "try:\n",
        "    page = requests.get(url)\n",
        "except:\n",
        "    print(\"Error al abrir la URL\")\n",
        "\n",
        "soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "\n",
        "# Buscamos el <div> correspondiente y sacamos su contenido:\n",
        "content = soup.find('div', {\"class\": \"a_c clearfix\"})\n",
        "article = []\n",
        "for i in content.find_all('p'):\n",
        "    article.append(i.text)\n",
        "\n",
        "article = ''.join(article)\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')  # tokenizador para separar en palabras\n",
        "stop_words = set(stopwords.words('spanish'))  # lista de stopwords\n",
        "stemmer = SnowballStemmer('spanish')  # lematizador para espa√±ol\n",
        "\n",
        "# Separar el texto en palabras\n",
        "words = tokenizer.tokenize(article.lower())\n",
        "\n",
        "# Filtrar stopwords y lematizar las palabras\n",
        "words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "\n",
        "# Analizar la subjetividad del texto\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "polarity_scores = sia.polarity_scores(' '.join(words))\n",
        "\n",
        "print(polarity_scores)\n",
        "print(words)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
